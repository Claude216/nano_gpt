{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMs-ZERO-to-HERO: NANO_GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1bc3bca7470>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "\n",
    "torch.manual_seed(1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Define the hyperparameters of GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig: \n",
    "    block_size: int = 512 # max sequence\n",
    "    batch_size: int = 12\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768   # hidden_dim, hidden_size; same as emb_szie for tie_embedding_weight\n",
    "    hidden_dim: int = n_embd\n",
    "    dropout: float = 0.1\n",
    "    head_size: int = n_embd // n_head\n",
    "    # vocab_size\n",
    "    # gpt2 official tokenizer\n",
    "    vocab_size: int = 50257"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Define the structure of GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. single head attention\n",
    "class SingleHeadAttention(nn.Module): \n",
    "    def __init__(self, config): \n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(config.n_embd, config.head_size)\n",
    "        self.value = nn.Linear(config.hidden_dim, config.head_size)\n",
    "        self.query = nn.Linear(config.hidden_dim, config.head_size)\n",
    "        self.head_size = config.head_size\n",
    "\n",
    "\n",
    "        # use register_buffer to register attention_mask\n",
    "        # it can save the memory and RAM, as no need to calculate the ** gradients ** \n",
    "        self.register_buffer(\n",
    "            \"attention_mask\",\n",
    "            # \"tril\" means lower triangle\n",
    "            # block_size is 512\n",
    "            torch.tril(\n",
    "                torch.ones(config.block_size, config.block_size)\n",
    "            )\n",
    "        )\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x): \n",
    "        batch_size, seq_len, hidden_dim = x.size()\n",
    "        # using the head size to calculate the similarity\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "        weight = q @ k.transpose(-2, -1)    # @ is short for torch.matmul\n",
    "        weight = weight.masked_fill(\n",
    "            self.attention_mask[:seq_len, :seq_len] == 0,\n",
    "            float('-inf')\n",
    "        )\n",
    "        # divide sqrt(d_k) when calcualting weight\n",
    "        weight = F.softmax(weight, dim=-1) / math.sqrt(self.head_size)\n",
    "\n",
    "        # dropout need to be after weight\n",
    "        # since dropout will drop the attention weight\n",
    "        weight = self.dropout(weight)\n",
    "        output = weight @ v\n",
    "        return output\n",
    "    \n",
    "# 2. multi head attention\n",
    "class MultiHeadAttention(nn.Module): \n",
    "    def  __init__(self, config) -> None:\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [\n",
    "                SingleHeadAttention(config)\n",
    "                for _ in range(config.n_head)\n",
    "            ]\n",
    "        )\n",
    "        self.proj = nn.Linear(config.hidden_dim, config.hidden_dim)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x): \n",
    "        output = torch.cat(\n",
    "            [h(x) for h in self.heads],\n",
    "            dim=-1\n",
    "        )\n",
    "        output = self.proj(output)\n",
    "        output = self.dropout(output)\n",
    "        return output\n",
    "\n",
    "# 3. feed forward (MLP)\n",
    "class FeedForward(nn.Module): \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(config.hidden_dim, 4 * config.hidden_dim), # swiglu # 8/3\n",
    "            nn.GELU(), # activation layer\n",
    "            nn.Linear(4 * config.hidden_dim, config.hidden_dim), \n",
    "            nn.Dropout(config.dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "# 4. block\n",
    "class Block(nn.Module): \n",
    "    def __init__(self, config): \n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(config)\n",
    "        self.ffn = FeedForward(config)\n",
    "        self.ln1 = nn.LayerNorm(config.hidden_dim)\n",
    "        self.ln2 = nn.LayerNorm(config.hidden_dim)\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = x + self.att(self.ln1(x))\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x \n",
    "    \n",
    "\n",
    "# 5. GPT\n",
    "class GPT(nn.Module): \n",
    "    def __init__(self, config): \n",
    "        super().__init__()\n",
    "        # (embedding, position, norm, mlp, block )\n",
    "        # position embdeeing from 0, 1, ... embedding -> ROPE\n",
    "        # norm: layer  norm -> rms norm\n",
    "        # mlp -> swiglu\n",
    "        # mha -> gqa\n",
    "        self.block_size = config.block_size\n",
    "        self.token_embedding_table = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(config.block_size, config.n_embd)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(config) for _ in range(config.n_layer)]\n",
    "        )\n",
    "        self.ln_final = nn.LayerNorm(config.n_embd)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False) # no bias as going to get softmax \n",
    "        # SLM model nowadays will use tie_weight to reduce parameters\n",
    "\n",
    "        # **important!!**\n",
    "        # in a small model, if the there are too many parameters related to the embedding\n",
    "        # the less knowledge the model would learn\n",
    "        # so we reduce the parameters related to embedding by tie_weight\n",
    "        # why it can be tied?\n",
    "        # linear(4 -> 8), the shape of weight is 8 * 4\n",
    "        self.token_embedding_table.weight = self.lm_head.weight\n",
    "\n",
    "    def __init__weights(self, module): # init like Gaussian Distribution\n",
    "        if isinstance(module, nn.Lienar): \n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None: \n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding): \n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None): \n",
    "        # idx: token ids\n",
    "        # target: target token ids\n",
    "        # --> shape of idx and target should be the same\n",
    "        batch, seq_len = idx.size() # (batch, seq_len)\n",
    "        token_emb = self.token_embedding_table(idx) # (batch, seq_len, n_embd)\n",
    "        pos_emb = self.position_embedding_table(\n",
    "            # ensure that the position encoding and the input idx to be on the \n",
    "            # same device\n",
    "            # why?\n",
    "            # we want to learn the position embedding table\n",
    "            # arrange would start from 0 to the seq_len\n",
    "            torch.arange(seq_len, device=idx.device)\n",
    "        )\n",
    "\n",
    "        # question: why we can sum up token embedding and position embedding? \n",
    "\n",
    "        x = token_emb + pos_emb     # shape is (batch, seq_len, n_embd)\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_final(x)\n",
    "        logits = self.lm_head(x)    # shape is (batch, seq_len, vocab_size)\n",
    "        if targets is None: \n",
    "            loss = None\n",
    "        else: \n",
    "            batch, seq_len, vocab_size = logits.size()\n",
    "            logits = logits.view(batch * seq_len, vocab_size)\n",
    "            targets = targets.view(batch * seq_len)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens): \n",
    "\n",
    "        # idx shape (batch, seq_len)\n",
    "        for _ in range(max_new_tokens): \n",
    "            idx_cond = idx if idx.size(1) <= self.block_size else idx[:, -self.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            \n",
    "            # shape: (batch, seq_len, vocab_size)\n",
    "            logits = logits[:, -1, :] # take the last one since tokens before that are input tokens\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # random sample\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # concat the new token with the prompt\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # shape: (batch, seq_len + 1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "        pass #TODO\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV. Construct the input dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Understand what the input be like (Most Important)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset): \n",
    "    def __init__(self, path, block_size=512): \n",
    "        # read the first 1000 lines \n",
    "        import tiktoken\n",
    "        self.enc = tiktoken.get_encoding(\"gpt2\")\n",
    "        self.block_size = block_size # pos max_len\n",
    "\n",
    "        self.encoded_data = []\n",
    "        # using special character to split different training text\n",
    "        # GPT's: <|endoftext|>\n",
    "        self.eos_token = self.enc.encode(\n",
    "            \"<|endoftext|>\", \n",
    "            allowed_special={\"<|endoftext|>\"}\n",
    "        )[0]\n",
    "        \n",
    "        self.max_lines = 1000\n",
    "\n",
    "        # Deserialization\n",
    "        import json\n",
    "\n",
    "        # processin the raw data to pkl or numpy format\n",
    "        # then load to the GPU memory\n",
    "\n",
    "        raw_data = []\n",
    "        with open(path, 'r', encoding='utf-8', errors='ignore') as f: \n",
    "            for i, line in enumerate(f): \n",
    "                if i >= self.max_lines: \n",
    "                    break\n",
    "                try: \n",
    "                    text = json.loads(line.strip())[\"text\"]\n",
    "                    raw_data.append(text)\n",
    "                except Exception as e: \n",
    "                    continue\n",
    "                        \n",
    "        full_encoded = []\n",
    "        for text in raw_data: \n",
    "            # combine all the text\n",
    "            # and divide them by the \n",
    "            # special character\n",
    "            encoded_text = self.enc.encode(text)\n",
    "            full_encoded.extend(encoded_text + [self.eos_token])\n",
    "        \n",
    "        # block_size = 512\n",
    "        # split the long text -> short (512)\n",
    "        for i in range(0, len(full_encoded), self.block_size):\n",
    "            chunk = full_encoded[i:i+self.block_size] # 512 # actually 513 for each line\n",
    "            if len(chunk) < self.block_size + 1: \n",
    "                # you can also drop it if it is not a multiple of 512\n",
    "                chunk = chunk + [self.eos_token] * (self.block_size + 1 - len(chunk))\n",
    "            self.encoded_data.append(chunk)\n",
    "\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.encoded_data)\n",
    "    \n",
    "    def __getitem__(self, idx): \n",
    "        chunk = self.encoded_data[idx]\n",
    "        x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(chunk[1:], dtype=torch.long)\n",
    "        return x, y\n",
    "    \n",
    "    def encode(self, text): \n",
    "        \"\"\"encode the text to token IDs\"\"\"\n",
    "        return self.enc.encode(text)\n",
    "    \n",
    "    def decod(self, ids): \n",
    "        \"\"\"decode the token IDs to text\"\"\"\n",
    "        return self.enc.decode(ids)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V. Run the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import chardet\n",
    "\n",
    "# with open('openwebtext.jsonl', 'rb') as f:\n",
    "#     raw_data = f.read(10000)  # Read first 10KB to detect encoding\n",
    "#     result = chardet.detect(raw_data)\n",
    "#     print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# import json\n",
    "\n",
    "# dataset_stream = load_dataset(\"openwebtext\", split=\"train\", streaming=True, trust_remote_code=True)\n",
    "\n",
    "# with open(\"./openwebtext.jsonl\", \"w\", encoding=\"utf-8\", errors='ignore') as f: \n",
    "\n",
    "#     for sample in dataset_stream:\n",
    "#         text_data = sample[\"text\"].replace(\"\\n\", \"\\\\n\")\n",
    "#         record = {\"text\": text_data}\n",
    "#         line_json = json.dumps(record, ensure_ascii=False)\n",
    "#         f.write(line_json + \"\\n\")\n",
    "\n",
    "        # print(sample[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data\n",
    "trainig_data_set_path = 'openwebtext.jsonl'\n",
    "train_dataset = MyDataset(trainig_data_set_path)\n",
    "\n",
    "# split traindataset to train and val\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [0.9, 0.1])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=12, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=12, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 124.046592 M\n"
     ]
    }
   ],
   "source": [
    "model = GPT(GPTConfig())\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "# print the parameters of the model\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params / 1e6} M\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "# 设置cosine 学习率\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Loss: 10.9832\n",
      "Epoch: 0, Train Loss: 0.0642, Val Loss: 10.0863\n",
      "Epoch: 1, Batch: 0, Loss: 10.0932\n",
      "Epoch: 1, Train Loss: 0.0590, Val Loss: 9.6039\n",
      "Epoch: 2, Batch: 0, Loss: 9.6466\n",
      "Epoch: 2, Train Loss: 0.0564, Val Loss: 9.3389\n",
      "Epoch: 3, Batch: 0, Loss: 9.3372\n",
      "Epoch: 3, Train Loss: 0.0546, Val Loss: 9.1214\n",
      "Epoch: 4, Batch: 0, Loss: 9.0803\n",
      "Epoch: 4, Train Loss: 0.0531, Val Loss: 8.9191\n",
      "Epoch: 5, Batch: 0, Loss: 8.9432\n",
      "Epoch: 5, Train Loss: 0.0523, Val Loss: 8.7255\n",
      "Epoch: 6, Batch: 0, Loss: 8.6562\n",
      "Epoch: 6, Train Loss: 0.0506, Val Loss: 8.5424\n",
      "Epoch: 7, Batch: 0, Loss: 8.5622\n",
      "Epoch: 7, Train Loss: 0.0501, Val Loss: 8.3758\n",
      "Epoch: 8, Batch: 0, Loss: 8.7010\n",
      "Epoch: 8, Train Loss: 0.0509, Val Loss: 8.2269\n",
      "Epoch: 9, Batch: 0, Loss: 8.2840\n",
      "Epoch: 9, Train Loss: 0.0484, Val Loss: 8.0938\n",
      "Epoch: 10, Batch: 0, Loss: 8.1063\n",
      "Epoch: 10, Train Loss: 0.0474, Val Loss: 7.9786\n",
      "Epoch: 11, Batch: 0, Loss: 8.0692\n",
      "Epoch: 11, Train Loss: 0.0472, Val Loss: 7.8826\n",
      "Epoch: 12, Batch: 0, Loss: 7.9318\n",
      "Epoch: 12, Train Loss: 0.0464, Val Loss: 7.8033\n",
      "Epoch: 13, Batch: 0, Loss: 7.8919\n",
      "Epoch: 13, Train Loss: 0.0462, Val Loss: 7.7394\n",
      "Epoch: 14, Batch: 0, Loss: 7.5164\n",
      "Epoch: 14, Train Loss: 0.0440, Val Loss: 7.6875\n",
      "Epoch: 15, Batch: 0, Loss: 7.8716\n",
      "Epoch: 15, Train Loss: 0.0460, Val Loss: 7.6483\n",
      "Epoch: 16, Batch: 0, Loss: 7.5424\n",
      "Epoch: 16, Train Loss: 0.0441, Val Loss: 7.6187\n",
      "Epoch: 17, Batch: 0, Loss: 7.6537\n",
      "Epoch: 17, Train Loss: 0.0448, Val Loss: 7.5991\n",
      "Epoch: 18, Batch: 0, Loss: 7.5885\n",
      "Epoch: 18, Train Loss: 0.0444, Val Loss: 7.5869\n",
      "Epoch: 19, Batch: 0, Loss: 7.5685\n",
      "Epoch: 19, Train Loss: 0.0443, Val Loss: 7.5816\n",
      "Epoch: 20, Batch: 0, Loss: 7.4468\n",
      "Epoch: 20, Train Loss: 0.0435, Val Loss: 7.5816\n",
      "Epoch: 21, Batch: 0, Loss: 7.8748\n",
      "Epoch: 21, Train Loss: 0.0461, Val Loss: 7.5848\n",
      "Epoch: 22, Batch: 0, Loss: 7.5747\n",
      "Epoch: 22, Train Loss: 0.0443, Val Loss: 7.5905\n",
      "Epoch: 23, Batch: 0, Loss: 7.4733\n",
      "Epoch: 23, Train Loss: 0.0437, Val Loss: 7.5967\n",
      "Epoch: 24, Batch: 0, Loss: 7.4066\n",
      "Epoch: 24, Train Loss: 0.0433, Val Loss: 7.6011\n",
      "Epoch: 25, Batch: 0, Loss: 7.6868\n",
      "Epoch: 25, Train Loss: 0.0450, Val Loss: 7.6037\n",
      "Epoch: 26, Batch: 0, Loss: 7.5682\n",
      "Epoch: 26, Train Loss: 0.0443, Val Loss: 7.6059\n",
      "Epoch: 27, Batch: 0, Loss: 7.6092\n",
      "Epoch: 27, Train Loss: 0.0445, Val Loss: 7.6073\n",
      "Epoch: 28, Batch: 0, Loss: 7.4983\n",
      "Epoch: 28, Train Loss: 0.0438, Val Loss: 7.6073\n",
      "Epoch: 29, Batch: 0, Loss: 7.4217\n",
      "Epoch: 29, Train Loss: 0.0434, Val Loss: 7.6057\n",
      "Epoch: 30, Batch: 0, Loss: 7.8515\n",
      "Epoch: 30, Train Loss: 0.0459, Val Loss: 7.6010\n",
      "Epoch: 31, Batch: 0, Loss: 7.7095\n",
      "Epoch: 31, Train Loss: 0.0451, Val Loss: 7.5946\n",
      "Epoch: 32, Batch: 0, Loss: 7.4852\n",
      "Epoch: 32, Train Loss: 0.0438, Val Loss: 7.5883\n",
      "Epoch: 33, Batch: 0, Loss: 7.6070\n",
      "Epoch: 33, Train Loss: 0.0445, Val Loss: 7.5808\n",
      "Epoch: 34, Batch: 0, Loss: 7.4038\n",
      "Epoch: 34, Train Loss: 0.0433, Val Loss: 7.5731\n",
      "Epoch: 35, Batch: 0, Loss: 7.6637\n",
      "Epoch: 35, Train Loss: 0.0448, Val Loss: 7.5654\n",
      "Epoch: 36, Batch: 0, Loss: 7.6609\n",
      "Epoch: 36, Train Loss: 0.0448, Val Loss: 7.5572\n",
      "Epoch: 37, Batch: 0, Loss: 7.5691\n",
      "Epoch: 37, Train Loss: 0.0443, Val Loss: 7.5521\n",
      "Epoch: 38, Batch: 0, Loss: 7.5559\n",
      "Epoch: 38, Train Loss: 0.0442, Val Loss: 7.5510\n",
      "Epoch: 39, Batch: 0, Loss: 7.4956\n",
      "Epoch: 39, Train Loss: 0.0438, Val Loss: 7.5473\n",
      "Epoch: 40, Batch: 0, Loss: 7.5866\n",
      "Epoch: 40, Train Loss: 0.0444, Val Loss: 7.5458\n",
      "Epoch: 41, Batch: 0, Loss: 7.3046\n",
      "Epoch: 41, Train Loss: 0.0427, Val Loss: 7.5449\n",
      "Epoch: 42, Batch: 0, Loss: 7.5670\n",
      "Epoch: 42, Train Loss: 0.0443, Val Loss: 7.5452\n",
      "Epoch: 43, Batch: 0, Loss: 7.3212\n",
      "Epoch: 43, Train Loss: 0.0428, Val Loss: 7.5463\n",
      "Epoch: 44, Batch: 0, Loss: 7.7474\n",
      "Epoch: 44, Train Loss: 0.0453, Val Loss: 7.5441\n",
      "Epoch: 45, Batch: 0, Loss: 7.3289\n",
      "Epoch: 45, Train Loss: 0.0429, Val Loss: 7.5404\n",
      "Epoch: 46, Batch: 0, Loss: 7.5246\n",
      "Epoch: 46, Train Loss: 0.0440, Val Loss: 7.5371\n",
      "Epoch: 47, Batch: 0, Loss: 7.6115\n",
      "Epoch: 47, Train Loss: 0.0445, Val Loss: 7.5348\n",
      "Epoch: 48, Batch: 0, Loss: 7.5058\n",
      "Epoch: 48, Train Loss: 0.0439, Val Loss: 7.5336\n",
      "Epoch: 49, Batch: 0, Loss: 7.5892\n",
      "Epoch: 49, Train Loss: 0.0444, Val Loss: 7.5318\n",
      "Epoch: 50, Batch: 0, Loss: 7.5512\n",
      "Epoch: 50, Train Loss: 0.0442, Val Loss: 7.5292\n",
      "Epoch: 51, Batch: 0, Loss: 7.4078\n",
      "Epoch: 51, Train Loss: 0.0433, Val Loss: 7.5265\n",
      "Epoch: 52, Batch: 0, Loss: 7.7881\n",
      "Epoch: 52, Train Loss: 0.0455, Val Loss: 7.5247\n",
      "Epoch: 53, Batch: 0, Loss: 7.4693\n",
      "Epoch: 53, Train Loss: 0.0437, Val Loss: 7.5234\n",
      "Epoch: 54, Batch: 0, Loss: 7.5117\n",
      "Epoch: 54, Train Loss: 0.0439, Val Loss: 7.5219\n",
      "Epoch: 55, Batch: 0, Loss: 7.3877\n",
      "Epoch: 55, Train Loss: 0.0432, Val Loss: 7.5190\n",
      "Epoch: 56, Batch: 0, Loss: 7.4991\n",
      "Epoch: 56, Train Loss: 0.0439, Val Loss: 7.5160\n",
      "Epoch: 57, Batch: 0, Loss: 7.4397\n",
      "Epoch: 57, Train Loss: 0.0435, Val Loss: 7.5131\n",
      "Epoch: 58, Batch: 0, Loss: 7.4803\n",
      "Epoch: 58, Train Loss: 0.0437, Val Loss: 7.5117\n",
      "Epoch: 59, Batch: 0, Loss: 7.7848\n",
      "Epoch: 59, Train Loss: 0.0455, Val Loss: 7.5109\n",
      "Epoch: 60, Batch: 0, Loss: 7.4651\n",
      "Epoch: 60, Train Loss: 0.0437, Val Loss: 7.5099\n",
      "Epoch: 61, Batch: 0, Loss: 7.8231\n",
      "Epoch: 61, Train Loss: 0.0457, Val Loss: 7.5089\n",
      "Epoch: 62, Batch: 0, Loss: 7.2241\n",
      "Epoch: 62, Train Loss: 0.0422, Val Loss: 7.5080\n",
      "Epoch: 63, Batch: 0, Loss: 7.2665\n",
      "Epoch: 63, Train Loss: 0.0425, Val Loss: 7.5091\n",
      "Epoch: 64, Batch: 0, Loss: 7.6285\n",
      "Epoch: 64, Train Loss: 0.0446, Val Loss: 7.5094\n",
      "Epoch: 65, Batch: 0, Loss: 7.4018\n",
      "Epoch: 65, Train Loss: 0.0433, Val Loss: 7.5089\n",
      "Epoch: 66, Batch: 0, Loss: 7.5250\n",
      "Epoch: 66, Train Loss: 0.0440, Val Loss: 7.5055\n",
      "Epoch: 67, Batch: 0, Loss: 7.3782\n",
      "Epoch: 67, Train Loss: 0.0431, Val Loss: 7.5034\n",
      "Epoch: 68, Batch: 0, Loss: 7.2966\n",
      "Epoch: 68, Train Loss: 0.0427, Val Loss: 7.5035\n",
      "Epoch: 69, Batch: 0, Loss: 7.2503\n",
      "Epoch: 69, Train Loss: 0.0424, Val Loss: 7.5050\n",
      "Epoch: 70, Batch: 0, Loss: 7.4898\n",
      "Epoch: 70, Train Loss: 0.0438, Val Loss: 7.5056\n",
      "Epoch: 71, Batch: 0, Loss: 7.4771\n",
      "Epoch: 71, Train Loss: 0.0437, Val Loss: 7.5059\n",
      "Epoch: 72, Batch: 0, Loss: 7.5923\n",
      "Epoch: 72, Train Loss: 0.0444, Val Loss: 7.5058\n",
      "Epoch: 73, Batch: 0, Loss: 7.2303\n",
      "Epoch: 73, Train Loss: 0.0423, Val Loss: 7.5054\n",
      "Epoch: 74, Batch: 0, Loss: 7.6283\n",
      "Epoch: 74, Train Loss: 0.0446, Val Loss: 7.5048\n",
      "Epoch: 75, Batch: 0, Loss: 7.2759\n",
      "Epoch: 75, Train Loss: 0.0425, Val Loss: 7.5044\n",
      "Epoch: 76, Batch: 0, Loss: 7.3116\n",
      "Epoch: 76, Train Loss: 0.0428, Val Loss: 7.5044\n",
      "Epoch: 77, Batch: 0, Loss: 7.6342\n",
      "Epoch: 77, Train Loss: 0.0446, Val Loss: 7.5036\n",
      "Epoch: 78, Batch: 0, Loss: 7.4404\n",
      "Epoch: 78, Train Loss: 0.0435, Val Loss: 7.5018\n",
      "Epoch: 79, Batch: 0, Loss: 7.6407\n",
      "Epoch: 79, Train Loss: 0.0447, Val Loss: 7.5002\n",
      "Epoch: 80, Batch: 0, Loss: 7.5903\n",
      "Epoch: 80, Train Loss: 0.0444, Val Loss: 7.4998\n",
      "Epoch: 81, Batch: 0, Loss: 7.3963\n",
      "Epoch: 81, Train Loss: 0.0433, Val Loss: 7.4993\n",
      "Epoch: 82, Batch: 0, Loss: 7.5101\n",
      "Epoch: 82, Train Loss: 0.0439, Val Loss: 7.4978\n",
      "Epoch: 83, Batch: 0, Loss: 7.4615\n",
      "Epoch: 83, Train Loss: 0.0436, Val Loss: 7.4959\n",
      "Epoch: 84, Batch: 0, Loss: 7.4607\n",
      "Epoch: 84, Train Loss: 0.0436, Val Loss: 7.4945\n",
      "Epoch: 85, Batch: 0, Loss: 7.4925\n",
      "Epoch: 85, Train Loss: 0.0438, Val Loss: 7.4947\n",
      "Epoch: 86, Batch: 0, Loss: 7.2711\n",
      "Epoch: 86, Train Loss: 0.0425, Val Loss: 7.4956\n",
      "Epoch: 87, Batch: 0, Loss: 7.4174\n",
      "Epoch: 87, Train Loss: 0.0434, Val Loss: 7.4960\n",
      "Epoch: 88, Batch: 0, Loss: 7.7285\n",
      "Epoch: 88, Train Loss: 0.0452, Val Loss: 7.4953\n",
      "Epoch: 89, Batch: 0, Loss: 7.6066\n",
      "Epoch: 89, Train Loss: 0.0445, Val Loss: 7.4946\n",
      "Epoch: 90, Batch: 0, Loss: 7.6482\n",
      "Epoch: 90, Train Loss: 0.0447, Val Loss: 7.4952\n",
      "Epoch: 91, Batch: 0, Loss: 7.5162\n",
      "Epoch: 91, Train Loss: 0.0440, Val Loss: 7.4963\n",
      "Epoch: 92, Batch: 0, Loss: 7.5191\n",
      "Epoch: 92, Train Loss: 0.0440, Val Loss: 7.4964\n",
      "Epoch: 93, Batch: 0, Loss: 7.6462\n",
      "Epoch: 93, Train Loss: 0.0447, Val Loss: 7.4972\n",
      "Epoch: 94, Batch: 0, Loss: 7.7270\n",
      "Epoch: 94, Train Loss: 0.0452, Val Loss: 7.4981\n",
      "Epoch: 95, Batch: 0, Loss: 7.3415\n",
      "Epoch: 95, Train Loss: 0.0429, Val Loss: 7.4981\n",
      "Epoch: 96, Batch: 0, Loss: 7.5333\n",
      "Epoch: 96, Train Loss: 0.0441, Val Loss: 7.4981\n",
      "Epoch: 97, Batch: 0, Loss: 7.6930\n",
      "Epoch: 97, Train Loss: 0.0450, Val Loss: 7.4987\n",
      "Epoch: 98, Batch: 0, Loss: 7.2394\n",
      "Epoch: 98, Train Loss: 0.0423, Val Loss: 7.5005\n",
      "Epoch: 99, Batch: 0, Loss: 7.3010\n",
      "Epoch: 99, Train Loss: 0.0427, Val Loss: 7.5024\n",
      "Epoch: 100, Batch: 0, Loss: 7.4629\n",
      "Epoch: 100, Train Loss: 0.0436, Val Loss: 7.5039\n",
      "Epoch: 101, Batch: 0, Loss: 7.4880\n",
      "Epoch: 101, Train Loss: 0.0438, Val Loss: 7.5032\n",
      "Epoch: 102, Batch: 0, Loss: 7.4500\n",
      "Epoch: 102, Train Loss: 0.0436, Val Loss: 7.5004\n",
      "Epoch: 103, Batch: 0, Loss: 7.3239\n",
      "Epoch: 103, Train Loss: 0.0428, Val Loss: 7.4985\n",
      "Epoch: 104, Batch: 0, Loss: 7.4397\n",
      "Epoch: 104, Train Loss: 0.0435, Val Loss: 7.4970\n",
      "Epoch: 105, Batch: 0, Loss: 7.5349\n",
      "Epoch: 105, Train Loss: 0.0441, Val Loss: 7.4964\n",
      "Epoch: 106, Batch: 0, Loss: 7.3602\n",
      "Epoch: 106, Train Loss: 0.0430, Val Loss: 7.4966\n",
      "Epoch: 107, Batch: 0, Loss: 7.5589\n",
      "Epoch: 107, Train Loss: 0.0442, Val Loss: 7.4965\n",
      "Epoch: 108, Batch: 0, Loss: 7.6310\n",
      "Epoch: 108, Train Loss: 0.0446, Val Loss: 7.4959\n",
      "Epoch: 109, Batch: 0, Loss: 7.3914\n",
      "Epoch: 109, Train Loss: 0.0432, Val Loss: 7.4956\n",
      "Epoch: 110, Batch: 0, Loss: 7.5672\n",
      "Epoch: 110, Train Loss: 0.0443, Val Loss: 7.4958\n",
      "Epoch: 111, Batch: 0, Loss: 7.2738\n",
      "Epoch: 111, Train Loss: 0.0425, Val Loss: 7.4961\n",
      "Epoch: 112, Batch: 0, Loss: 7.3138\n",
      "Epoch: 112, Train Loss: 0.0428, Val Loss: 7.4979\n",
      "Epoch: 113, Batch: 0, Loss: 7.5094\n",
      "Epoch: 113, Train Loss: 0.0439, Val Loss: 7.4988\n",
      "Epoch: 114, Batch: 0, Loss: 7.3579\n",
      "Epoch: 114, Train Loss: 0.0430, Val Loss: 7.4974\n",
      "Epoch: 115, Batch: 0, Loss: 7.5593\n",
      "Epoch: 115, Train Loss: 0.0442, Val Loss: 7.4944\n",
      "Epoch: 116, Batch: 0, Loss: 7.3523\n",
      "Epoch: 116, Train Loss: 0.0430, Val Loss: 7.4924\n",
      "Epoch: 117, Batch: 0, Loss: 7.3764\n",
      "Epoch: 117, Train Loss: 0.0431, Val Loss: 7.4919\n",
      "Epoch: 118, Batch: 0, Loss: 7.4086\n",
      "Epoch: 118, Train Loss: 0.0433, Val Loss: 7.4919\n",
      "Epoch: 119, Batch: 0, Loss: 7.3926\n",
      "Epoch: 119, Train Loss: 0.0432, Val Loss: 7.4919\n",
      "Epoch: 120, Batch: 0, Loss: 7.4266\n",
      "Epoch: 120, Train Loss: 0.0434, Val Loss: 7.4916\n",
      "Epoch: 121, Batch: 0, Loss: 7.3858\n",
      "Epoch: 121, Train Loss: 0.0432, Val Loss: 7.4919\n",
      "Epoch: 122, Batch: 0, Loss: 7.4751\n",
      "Epoch: 122, Train Loss: 0.0437, Val Loss: 7.4921\n",
      "Epoch: 123, Batch: 0, Loss: 7.6661\n",
      "Epoch: 123, Train Loss: 0.0448, Val Loss: 7.4921\n",
      "Epoch: 124, Batch: 0, Loss: 7.3339\n",
      "Epoch: 124, Train Loss: 0.0429, Val Loss: 7.4918\n",
      "Epoch: 125, Batch: 0, Loss: 7.4838\n",
      "Epoch: 125, Train Loss: 0.0438, Val Loss: 7.4914\n",
      "Epoch: 126, Batch: 0, Loss: 7.4960\n",
      "Epoch: 126, Train Loss: 0.0438, Val Loss: 7.4908\n",
      "Epoch: 127, Batch: 0, Loss: 7.6678\n",
      "Epoch: 127, Train Loss: 0.0448, Val Loss: 7.4899\n",
      "Epoch: 128, Batch: 0, Loss: 7.3878\n",
      "Epoch: 128, Train Loss: 0.0432, Val Loss: 7.4902\n",
      "Epoch: 129, Batch: 0, Loss: 7.4618\n",
      "Epoch: 129, Train Loss: 0.0436, Val Loss: 7.4904\n",
      "Epoch: 130, Batch: 0, Loss: 7.4306\n",
      "Epoch: 130, Train Loss: 0.0435, Val Loss: 7.4901\n",
      "Epoch: 131, Batch: 0, Loss: 7.4449\n",
      "Epoch: 131, Train Loss: 0.0435, Val Loss: 7.4889\n",
      "Epoch: 132, Batch: 0, Loss: 7.5326\n",
      "Epoch: 132, Train Loss: 0.0441, Val Loss: 7.4877\n",
      "Epoch: 133, Batch: 0, Loss: 7.3084\n",
      "Epoch: 133, Train Loss: 0.0427, Val Loss: 7.4876\n",
      "Epoch: 134, Batch: 0, Loss: 7.1914\n",
      "Epoch: 134, Train Loss: 0.0421, Val Loss: 7.4877\n",
      "Epoch: 135, Batch: 0, Loss: 7.7812\n",
      "Epoch: 135, Train Loss: 0.0455, Val Loss: 7.4858\n",
      "Epoch: 136, Batch: 0, Loss: 7.4690\n",
      "Epoch: 136, Train Loss: 0.0437, Val Loss: 7.4845\n",
      "Epoch: 137, Batch: 0, Loss: 7.3297\n",
      "Epoch: 137, Train Loss: 0.0429, Val Loss: 7.4858\n",
      "Epoch: 138, Batch: 0, Loss: 7.3077\n",
      "Epoch: 138, Train Loss: 0.0427, Val Loss: 7.4884\n",
      "Epoch: 139, Batch: 0, Loss: 7.6713\n",
      "Epoch: 139, Train Loss: 0.0449, Val Loss: 7.4894\n",
      "Epoch: 140, Batch: 0, Loss: 7.2914\n",
      "Epoch: 140, Train Loss: 0.0426, Val Loss: 7.4883\n",
      "Epoch: 141, Batch: 0, Loss: 7.6999\n",
      "Epoch: 141, Train Loss: 0.0450, Val Loss: 7.4852\n",
      "Epoch: 142, Batch: 0, Loss: 7.5469\n",
      "Epoch: 142, Train Loss: 0.0441, Val Loss: 7.4838\n",
      "Epoch: 143, Batch: 0, Loss: 7.3993\n",
      "Epoch: 143, Train Loss: 0.0433, Val Loss: 7.4840\n",
      "Epoch: 144, Batch: 0, Loss: 7.2870\n",
      "Epoch: 144, Train Loss: 0.0426, Val Loss: 7.4846\n",
      "Epoch: 145, Batch: 0, Loss: 7.4532\n",
      "Epoch: 145, Train Loss: 0.0436, Val Loss: 7.4860\n",
      "Epoch: 146, Batch: 0, Loss: 7.4403\n",
      "Epoch: 146, Train Loss: 0.0435, Val Loss: 7.4868\n",
      "Epoch: 147, Batch: 0, Loss: 7.8729\n",
      "Epoch: 147, Train Loss: 0.0460, Val Loss: 7.4854\n",
      "Epoch: 148, Batch: 0, Loss: 7.4248\n",
      "Epoch: 148, Train Loss: 0.0434, Val Loss: 7.4867\n",
      "Epoch: 149, Batch: 0, Loss: 7.3396\n",
      "Epoch: 149, Train Loss: 0.0429, Val Loss: 7.4924\n",
      "Epoch: 150, Batch: 0, Loss: 7.4941\n",
      "Epoch: 150, Train Loss: 0.0438, Val Loss: 7.4956\n",
      "Epoch: 151, Batch: 0, Loss: 7.3294\n",
      "Epoch: 151, Train Loss: 0.0429, Val Loss: 7.4966\n",
      "Epoch: 152, Batch: 0, Loss: 7.1992\n",
      "Epoch: 152, Train Loss: 0.0421, Val Loss: 7.4951\n",
      "Epoch: 153, Batch: 0, Loss: 7.4549\n",
      "Epoch: 153, Train Loss: 0.0436, Val Loss: 7.4913\n",
      "Epoch: 154, Batch: 0, Loss: 7.3882\n",
      "Epoch: 154, Train Loss: 0.0432, Val Loss: 7.4910\n",
      "Epoch: 155, Batch: 0, Loss: 7.3884\n",
      "Epoch: 155, Train Loss: 0.0432, Val Loss: 7.4911\n",
      "Epoch: 156, Batch: 0, Loss: 7.6495\n",
      "Epoch: 156, Train Loss: 0.0447, Val Loss: 7.4902\n",
      "Epoch: 157, Batch: 0, Loss: 7.4460\n",
      "Epoch: 157, Train Loss: 0.0435, Val Loss: 7.4899\n",
      "Epoch: 158, Batch: 0, Loss: 7.5212\n",
      "Epoch: 158, Train Loss: 0.0440, Val Loss: 7.4905\n",
      "Epoch: 159, Batch: 0, Loss: 7.4185\n",
      "Epoch: 159, Train Loss: 0.0434, Val Loss: 7.4911\n",
      "Epoch: 160, Batch: 0, Loss: 7.4916\n",
      "Epoch: 160, Train Loss: 0.0438, Val Loss: 7.4903\n",
      "Epoch: 161, Batch: 0, Loss: 7.4173\n",
      "Epoch: 161, Train Loss: 0.0434, Val Loss: 7.4892\n",
      "Epoch: 162, Batch: 0, Loss: 7.3100\n",
      "Epoch: 162, Train Loss: 0.0427, Val Loss: 7.4892\n",
      "Epoch: 163, Batch: 0, Loss: 7.6216\n",
      "Epoch: 163, Train Loss: 0.0446, Val Loss: 7.4892\n",
      "Epoch: 164, Batch: 0, Loss: 7.5080\n",
      "Epoch: 164, Train Loss: 0.0439, Val Loss: 7.4878\n",
      "Epoch: 165, Batch: 0, Loss: 7.4575\n",
      "Epoch: 165, Train Loss: 0.0436, Val Loss: 7.4856\n",
      "Epoch: 166, Batch: 0, Loss: 7.4425\n",
      "Epoch: 166, Train Loss: 0.0435, Val Loss: 7.4851\n",
      "Epoch: 167, Batch: 0, Loss: 7.2166\n",
      "Epoch: 167, Train Loss: 0.0422, Val Loss: 7.4886\n",
      "Epoch: 168, Batch: 0, Loss: 7.4190\n",
      "Epoch: 168, Train Loss: 0.0434, Val Loss: 7.4947\n",
      "Epoch: 169, Batch: 0, Loss: 7.3141\n",
      "Epoch: 169, Train Loss: 0.0428, Val Loss: 7.4992\n",
      "Epoch: 170, Batch: 0, Loss: 7.3511\n",
      "Epoch: 170, Train Loss: 0.0430, Val Loss: 7.4965\n",
      "Epoch: 171, Batch: 0, Loss: 7.4991\n",
      "Epoch: 171, Train Loss: 0.0439, Val Loss: 7.4887\n",
      "Epoch: 172, Batch: 0, Loss: 7.2570\n",
      "Epoch: 172, Train Loss: 0.0424, Val Loss: 7.4855\n",
      "Epoch: 173, Batch: 0, Loss: 7.5831\n",
      "Epoch: 173, Train Loss: 0.0443, Val Loss: 7.4844\n",
      "Epoch: 174, Batch: 0, Loss: 7.4023\n",
      "Epoch: 174, Train Loss: 0.0433, Val Loss: 7.4862\n",
      "Epoch: 175, Batch: 0, Loss: 7.3717\n",
      "Epoch: 175, Train Loss: 0.0431, Val Loss: 7.4887\n",
      "Epoch: 176, Batch: 0, Loss: 7.2401\n",
      "Epoch: 176, Train Loss: 0.0423, Val Loss: 7.4907\n",
      "Epoch: 177, Batch: 0, Loss: 7.4032\n",
      "Epoch: 177, Train Loss: 0.0433, Val Loss: 7.4902\n",
      "Epoch: 178, Batch: 0, Loss: 7.2657\n",
      "Epoch: 178, Train Loss: 0.0425, Val Loss: 7.4876\n",
      "Epoch: 179, Batch: 0, Loss: 7.5770\n",
      "Epoch: 179, Train Loss: 0.0443, Val Loss: 7.4854\n",
      "Epoch: 180, Batch: 0, Loss: 7.4495\n",
      "Epoch: 180, Train Loss: 0.0436, Val Loss: 7.4843\n",
      "Epoch: 181, Batch: 0, Loss: 7.4676\n",
      "Epoch: 181, Train Loss: 0.0437, Val Loss: 7.4848\n",
      "Epoch: 182, Batch: 0, Loss: 7.2988\n",
      "Epoch: 182, Train Loss: 0.0427, Val Loss: 7.4869\n",
      "Epoch: 183, Batch: 0, Loss: 7.4457\n",
      "Epoch: 183, Train Loss: 0.0435, Val Loss: 7.4908\n",
      "Epoch: 184, Batch: 0, Loss: 7.4948\n",
      "Epoch: 184, Train Loss: 0.0438, Val Loss: 7.4943\n",
      "Epoch: 185, Batch: 0, Loss: 7.2976\n",
      "Epoch: 185, Train Loss: 0.0427, Val Loss: 7.4953\n",
      "Epoch: 186, Batch: 0, Loss: 7.5479\n",
      "Epoch: 186, Train Loss: 0.0441, Val Loss: 7.4902\n",
      "Epoch: 187, Batch: 0, Loss: 7.3543\n",
      "Epoch: 187, Train Loss: 0.0430, Val Loss: 7.4843\n",
      "Epoch: 188, Batch: 0, Loss: 7.3720\n",
      "Epoch: 188, Train Loss: 0.0431, Val Loss: 7.4813\n",
      "Epoch: 189, Batch: 0, Loss: 7.3325\n",
      "Epoch: 189, Train Loss: 0.0429, Val Loss: 7.4813\n",
      "Epoch: 190, Batch: 0, Loss: 7.3720\n",
      "Epoch: 190, Train Loss: 0.0431, Val Loss: 7.4829\n",
      "Epoch: 191, Batch: 0, Loss: 7.3635\n",
      "Epoch: 191, Train Loss: 0.0431, Val Loss: 7.4847\n",
      "Epoch: 192, Batch: 0, Loss: 7.3194\n",
      "Epoch: 192, Train Loss: 0.0428, Val Loss: 7.4845\n",
      "Epoch: 193, Batch: 0, Loss: 7.6239\n",
      "Epoch: 193, Train Loss: 0.0446, Val Loss: 7.4827\n",
      "Epoch: 194, Batch: 0, Loss: 7.5638\n",
      "Epoch: 194, Train Loss: 0.0442, Val Loss: 7.4813\n",
      "Epoch: 195, Batch: 0, Loss: 7.4693\n",
      "Epoch: 195, Train Loss: 0.0437, Val Loss: 7.4814\n",
      "Epoch: 196, Batch: 0, Loss: 7.3923\n",
      "Epoch: 196, Train Loss: 0.0432, Val Loss: 7.4812\n",
      "Epoch: 197, Batch: 0, Loss: 7.4192\n",
      "Epoch: 197, Train Loss: 0.0434, Val Loss: 7.4809\n",
      "Epoch: 198, Batch: 0, Loss: 7.3868\n",
      "Epoch: 198, Train Loss: 0.0432, Val Loss: 7.4816\n",
      "Epoch: 199, Batch: 0, Loss: 7.4976\n",
      "Epoch: 199, Train Loss: 0.0438, Val Loss: 7.4821\n",
      "Epoch: 200, Batch: 0, Loss: 7.4380\n",
      "Epoch: 200, Train Loss: 0.0435, Val Loss: 7.4831\n",
      "Epoch: 201, Batch: 0, Loss: 7.5920\n",
      "Epoch: 201, Train Loss: 0.0444, Val Loss: 7.4823\n",
      "Epoch: 202, Batch: 0, Loss: 7.6850\n",
      "Epoch: 202, Train Loss: 0.0449, Val Loss: 7.4808\n",
      "Epoch: 203, Batch: 0, Loss: 7.4949\n",
      "Epoch: 203, Train Loss: 0.0438, Val Loss: 7.4798\n",
      "Epoch: 204, Batch: 0, Loss: 7.6569\n",
      "Epoch: 204, Train Loss: 0.0448, Val Loss: 7.4798\n",
      "Epoch: 205, Batch: 0, Loss: 7.5318\n",
      "Epoch: 205, Train Loss: 0.0440, Val Loss: 7.4807\n",
      "Epoch: 206, Batch: 0, Loss: 7.3993\n",
      "Epoch: 206, Train Loss: 0.0433, Val Loss: 7.4806\n",
      "Epoch: 207, Batch: 0, Loss: 7.4750\n",
      "Epoch: 207, Train Loss: 0.0437, Val Loss: 7.4798\n",
      "Epoch: 208, Batch: 0, Loss: 7.3638\n",
      "Epoch: 208, Train Loss: 0.0431, Val Loss: 7.4795\n",
      "Epoch: 209, Batch: 0, Loss: 7.3691\n",
      "Epoch: 209, Train Loss: 0.0431, Val Loss: 7.4805\n",
      "Epoch: 210, Batch: 0, Loss: 7.3774\n",
      "Epoch: 210, Train Loss: 0.0431, Val Loss: 7.4825\n",
      "Epoch: 211, Batch: 0, Loss: 7.2383\n",
      "Epoch: 211, Train Loss: 0.0423, Val Loss: 7.4872\n",
      "Epoch: 212, Batch: 0, Loss: 7.3472\n",
      "Epoch: 212, Train Loss: 0.0430, Val Loss: 7.4871\n",
      "Epoch: 213, Batch: 0, Loss: 7.3989\n",
      "Epoch: 213, Train Loss: 0.0433, Val Loss: 7.4838\n",
      "Epoch: 214, Batch: 0, Loss: 7.3236\n",
      "Epoch: 214, Train Loss: 0.0428, Val Loss: 7.4813\n",
      "Epoch: 215, Batch: 0, Loss: 7.4490\n",
      "Epoch: 215, Train Loss: 0.0436, Val Loss: 7.4797\n",
      "Epoch: 216, Batch: 0, Loss: 7.3825\n",
      "Epoch: 216, Train Loss: 0.0432, Val Loss: 7.4798\n",
      "Epoch: 217, Batch: 0, Loss: 7.4299\n",
      "Epoch: 217, Train Loss: 0.0434, Val Loss: 7.4800\n",
      "Epoch: 218, Batch: 0, Loss: 7.4085\n",
      "Epoch: 218, Train Loss: 0.0433, Val Loss: 7.4800\n",
      "Epoch: 219, Batch: 0, Loss: 7.3522\n",
      "Epoch: 219, Train Loss: 0.0430, Val Loss: 7.4791\n",
      "Epoch: 220, Batch: 0, Loss: 7.6424\n",
      "Epoch: 220, Train Loss: 0.0447, Val Loss: 7.4788\n",
      "Epoch: 221, Batch: 0, Loss: 7.4773\n",
      "Epoch: 221, Train Loss: 0.0437, Val Loss: 7.4781\n",
      "Epoch: 222, Batch: 0, Loss: 7.4297\n",
      "Epoch: 222, Train Loss: 0.0434, Val Loss: 7.4771\n",
      "Epoch: 223, Batch: 0, Loss: 7.3414\n",
      "Epoch: 223, Train Loss: 0.0429, Val Loss: 7.4763\n",
      "Epoch: 224, Batch: 0, Loss: 7.3873\n",
      "Epoch: 224, Train Loss: 0.0432, Val Loss: 7.4757\n",
      "Epoch: 225, Batch: 0, Loss: 7.3562\n",
      "Epoch: 225, Train Loss: 0.0430, Val Loss: 7.4755\n",
      "Epoch: 226, Batch: 0, Loss: 7.5260\n",
      "Epoch: 226, Train Loss: 0.0440, Val Loss: 7.4757\n",
      "Epoch: 227, Batch: 0, Loss: 7.2379\n",
      "Epoch: 227, Train Loss: 0.0423, Val Loss: 7.4755\n",
      "Epoch: 228, Batch: 0, Loss: 7.4284\n",
      "Epoch: 228, Train Loss: 0.0434, Val Loss: 7.4745\n",
      "Epoch: 229, Batch: 0, Loss: 7.2375\n",
      "Epoch: 229, Train Loss: 0.0423, Val Loss: 7.4738\n",
      "Epoch: 230, Batch: 0, Loss: 7.5259\n",
      "Epoch: 230, Train Loss: 0.0440, Val Loss: 7.4731\n",
      "Epoch: 231, Batch: 0, Loss: 7.4128\n",
      "Epoch: 231, Train Loss: 0.0433, Val Loss: 7.4732\n",
      "Epoch: 232, Batch: 0, Loss: 7.4793\n",
      "Epoch: 232, Train Loss: 0.0437, Val Loss: 7.4734\n",
      "Epoch: 233, Batch: 0, Loss: 7.4998\n",
      "Epoch: 233, Train Loss: 0.0439, Val Loss: 7.4726\n",
      "Epoch: 234, Batch: 0, Loss: 7.2224\n",
      "Epoch: 234, Train Loss: 0.0422, Val Loss: 7.4717\n",
      "Epoch: 235, Batch: 0, Loss: 7.2420\n",
      "Epoch: 235, Train Loss: 0.0424, Val Loss: 7.4708\n",
      "Epoch: 236, Batch: 0, Loss: 7.6143\n",
      "Epoch: 236, Train Loss: 0.0445, Val Loss: 7.4704\n",
      "Epoch: 237, Batch: 0, Loss: 7.3857\n",
      "Epoch: 237, Train Loss: 0.0432, Val Loss: 7.4703\n",
      "Epoch: 238, Batch: 0, Loss: 7.4459\n",
      "Epoch: 238, Train Loss: 0.0435, Val Loss: 7.4713\n",
      "Epoch: 239, Batch: 0, Loss: 7.5956\n",
      "Epoch: 239, Train Loss: 0.0444, Val Loss: 7.4701\n",
      "Epoch: 240, Batch: 0, Loss: 7.4132\n",
      "Epoch: 240, Train Loss: 0.0434, Val Loss: 7.4695\n",
      "Epoch: 241, Batch: 0, Loss: 7.4925\n",
      "Epoch: 241, Train Loss: 0.0438, Val Loss: 7.4685\n",
      "Epoch: 242, Batch: 0, Loss: 7.5515\n",
      "Epoch: 242, Train Loss: 0.0442, Val Loss: 7.4694\n",
      "Epoch: 243, Batch: 0, Loss: 7.4637\n",
      "Epoch: 243, Train Loss: 0.0436, Val Loss: 7.4695\n",
      "Epoch: 244, Batch: 0, Loss: 7.4731\n",
      "Epoch: 244, Train Loss: 0.0437, Val Loss: 7.4687\n",
      "Epoch: 245, Batch: 0, Loss: 7.5112\n",
      "Epoch: 245, Train Loss: 0.0439, Val Loss: 7.4683\n",
      "Epoch: 246, Batch: 0, Loss: 7.2769\n",
      "Epoch: 246, Train Loss: 0.0426, Val Loss: 7.4685\n",
      "Epoch: 247, Batch: 0, Loss: 7.4757\n",
      "Epoch: 247, Train Loss: 0.0437, Val Loss: 7.4682\n",
      "Epoch: 248, Batch: 0, Loss: 7.5767\n",
      "Epoch: 248, Train Loss: 0.0443, Val Loss: 7.4662\n",
      "Epoch: 249, Batch: 0, Loss: 7.2264\n",
      "Epoch: 249, Train Loss: 0.0423, Val Loss: 7.4657\n",
      "Epoch: 250, Batch: 0, Loss: 7.3821\n",
      "Epoch: 250, Train Loss: 0.0432, Val Loss: 7.4650\n",
      "Epoch: 251, Batch: 0, Loss: 7.4856\n",
      "Epoch: 251, Train Loss: 0.0438, Val Loss: 7.4663\n",
      "Epoch: 252, Batch: 0, Loss: 7.3434\n",
      "Epoch: 252, Train Loss: 0.0429, Val Loss: 7.4650\n",
      "Epoch: 253, Batch: 0, Loss: 7.5436\n",
      "Epoch: 253, Train Loss: 0.0441, Val Loss: 7.4679\n",
      "Epoch: 254, Batch: 0, Loss: 7.5402\n",
      "Epoch: 254, Train Loss: 0.0441, Val Loss: 7.4650\n",
      "Epoch: 255, Batch: 0, Loss: 7.5214\n",
      "Epoch: 255, Train Loss: 0.0440, Val Loss: 7.4648\n",
      "Epoch: 256, Batch: 0, Loss: 7.2874\n",
      "Epoch: 256, Train Loss: 0.0426, Val Loss: 7.4712\n",
      "Epoch: 257, Batch: 0, Loss: 7.4938\n",
      "Epoch: 257, Train Loss: 0.0438, Val Loss: 7.4600\n",
      "Epoch: 258, Batch: 0, Loss: 7.3596\n",
      "Epoch: 258, Train Loss: 0.0430, Val Loss: 7.4603\n",
      "Epoch: 259, Batch: 0, Loss: 7.3770\n",
      "Epoch: 259, Train Loss: 0.0431, Val Loss: 7.4588\n",
      "Epoch: 260, Batch: 0, Loss: 7.4246\n",
      "Epoch: 260, Train Loss: 0.0434, Val Loss: 7.4559\n",
      "Epoch: 261, Batch: 0, Loss: 7.5952\n",
      "Epoch: 261, Train Loss: 0.0444, Val Loss: 7.4544\n",
      "Epoch: 262, Batch: 0, Loss: 7.2659\n",
      "Epoch: 262, Train Loss: 0.0425, Val Loss: 7.4523\n",
      "Epoch: 263, Batch: 0, Loss: 7.3950\n",
      "Epoch: 263, Train Loss: 0.0432, Val Loss: 7.4489\n",
      "Epoch: 264, Batch: 0, Loss: 7.5468\n",
      "Epoch: 264, Train Loss: 0.0441, Val Loss: 7.4502\n",
      "Epoch: 265, Batch: 0, Loss: 7.4141\n",
      "Epoch: 265, Train Loss: 0.0434, Val Loss: 7.4443\n",
      "Epoch: 266, Batch: 0, Loss: 7.2874\n",
      "Epoch: 266, Train Loss: 0.0426, Val Loss: 7.4470\n",
      "Epoch: 267, Batch: 0, Loss: 7.4112\n",
      "Epoch: 267, Train Loss: 0.0433, Val Loss: 7.4355\n",
      "Epoch: 268, Batch: 0, Loss: 7.4119\n",
      "Epoch: 268, Train Loss: 0.0433, Val Loss: 7.4408\n",
      "Epoch: 269, Batch: 0, Loss: 7.3409\n",
      "Epoch: 269, Train Loss: 0.0429, Val Loss: 7.4308\n",
      "Epoch: 270, Batch: 0, Loss: 7.3323\n",
      "Epoch: 270, Train Loss: 0.0429, Val Loss: 7.4282\n",
      "Epoch: 271, Batch: 0, Loss: 7.2239\n",
      "Epoch: 271, Train Loss: 0.0422, Val Loss: 7.4183\n",
      "Epoch: 272, Batch: 0, Loss: 7.4682\n",
      "Epoch: 272, Train Loss: 0.0437, Val Loss: 7.4246\n",
      "Epoch: 273, Batch: 0, Loss: 7.6345\n",
      "Epoch: 273, Train Loss: 0.0446, Val Loss: 7.4161\n",
      "Epoch: 274, Batch: 0, Loss: 7.2227\n",
      "Epoch: 274, Train Loss: 0.0422, Val Loss: 7.4114\n",
      "Epoch: 275, Batch: 0, Loss: 7.4573\n",
      "Epoch: 275, Train Loss: 0.0436, Val Loss: 7.4027\n",
      "Epoch: 276, Batch: 0, Loss: 7.3939\n",
      "Epoch: 276, Train Loss: 0.0432, Val Loss: 7.4039\n",
      "Epoch: 277, Batch: 0, Loss: 7.0945\n",
      "Epoch: 277, Train Loss: 0.0415, Val Loss: 7.4470\n",
      "Epoch: 278, Batch: 0, Loss: 7.4012\n",
      "Epoch: 278, Train Loss: 0.0433, Val Loss: 7.4287\n",
      "Epoch: 279, Batch: 0, Loss: 7.2599\n",
      "Epoch: 279, Train Loss: 0.0425, Val Loss: 7.4481\n",
      "Epoch: 280, Batch: 0, Loss: 7.4199\n",
      "Epoch: 280, Train Loss: 0.0434, Val Loss: 7.4342\n",
      "Epoch: 281, Batch: 0, Loss: 7.3545\n",
      "Epoch: 281, Train Loss: 0.0430, Val Loss: 7.4392\n",
      "Epoch: 282, Batch: 0, Loss: 7.4632\n",
      "Epoch: 282, Train Loss: 0.0436, Val Loss: 7.4285\n",
      "Epoch: 283, Batch: 0, Loss: 7.3429\n",
      "Epoch: 283, Train Loss: 0.0429, Val Loss: 7.4191\n",
      "Epoch: 284, Batch: 0, Loss: 7.4331\n",
      "Epoch: 284, Train Loss: 0.0435, Val Loss: 7.4186\n",
      "Epoch: 285, Batch: 0, Loss: 7.5403\n",
      "Epoch: 285, Train Loss: 0.0441, Val Loss: 7.4222\n",
      "Epoch: 286, Batch: 0, Loss: 7.4811\n",
      "Epoch: 286, Train Loss: 0.0437, Val Loss: 7.4205\n",
      "Epoch: 287, Batch: 0, Loss: 7.2756\n",
      "Epoch: 287, Train Loss: 0.0425, Val Loss: 7.4176\n",
      "Epoch: 288, Batch: 0, Loss: 7.7065\n",
      "Epoch: 288, Train Loss: 0.0451, Val Loss: 7.4099\n",
      "Epoch: 289, Batch: 0, Loss: 7.3893\n",
      "Epoch: 289, Train Loss: 0.0432, Val Loss: 7.4042\n",
      "Epoch: 290, Batch: 0, Loss: 7.3512\n",
      "Epoch: 290, Train Loss: 0.0430, Val Loss: 7.3988\n",
      "Epoch: 291, Batch: 0, Loss: 7.3452\n",
      "Epoch: 291, Train Loss: 0.0430, Val Loss: 7.3932\n",
      "Epoch: 292, Batch: 0, Loss: 7.1415\n",
      "Epoch: 292, Train Loss: 0.0418, Val Loss: 7.3913\n",
      "Epoch: 293, Batch: 0, Loss: 7.3823\n",
      "Epoch: 293, Train Loss: 0.0432, Val Loss: 7.3884\n",
      "Epoch: 294, Batch: 0, Loss: 7.3036\n",
      "Epoch: 294, Train Loss: 0.0427, Val Loss: 7.3877\n",
      "Epoch: 295, Batch: 0, Loss: 7.3391\n",
      "Epoch: 295, Train Loss: 0.0429, Val Loss: 7.3813\n",
      "Epoch: 296, Batch: 0, Loss: 7.3220\n",
      "Epoch: 296, Train Loss: 0.0428, Val Loss: 7.3773\n",
      "Epoch: 297, Batch: 0, Loss: 7.4546\n",
      "Epoch: 297, Train Loss: 0.0436, Val Loss: 7.3767\n",
      "Epoch: 298, Batch: 0, Loss: 7.2383\n",
      "Epoch: 298, Train Loss: 0.0423, Val Loss: 7.3761\n",
      "Epoch: 299, Batch: 0, Loss: 7.4153\n",
      "Epoch: 299, Train Loss: 0.0434, Val Loss: 7.3790\n",
      "Epoch: 300, Batch: 0, Loss: 7.1654\n",
      "Epoch: 300, Train Loss: 0.0419, Val Loss: 7.3753\n",
      "Epoch: 301, Batch: 0, Loss: 7.3725\n",
      "Epoch: 301, Train Loss: 0.0431, Val Loss: 7.3752\n",
      "Epoch: 302, Batch: 0, Loss: 7.2991\n",
      "Epoch: 302, Train Loss: 0.0427, Val Loss: 7.3757\n",
      "Epoch: 303, Batch: 0, Loss: 7.4018\n",
      "Epoch: 303, Train Loss: 0.0433, Val Loss: 7.3799\n",
      "Epoch: 304, Batch: 0, Loss: 7.6003\n",
      "Epoch: 304, Train Loss: 0.0444, Val Loss: 7.3818\n",
      "Epoch: 305, Batch: 0, Loss: 7.4246\n",
      "Epoch: 305, Train Loss: 0.0434, Val Loss: 7.3838\n",
      "Epoch: 306, Batch: 0, Loss: 7.3032\n",
      "Epoch: 306, Train Loss: 0.0427, Val Loss: 7.3804\n",
      "Epoch: 307, Batch: 0, Loss: 7.3308\n",
      "Epoch: 307, Train Loss: 0.0429, Val Loss: 7.3787\n",
      "Epoch: 308, Batch: 0, Loss: 7.4890\n",
      "Epoch: 308, Train Loss: 0.0438, Val Loss: 7.3787\n",
      "Epoch: 309, Batch: 0, Loss: 7.4632\n",
      "Epoch: 309, Train Loss: 0.0436, Val Loss: 7.3837\n",
      "Epoch: 310, Batch: 0, Loss: 7.1504\n",
      "Epoch: 310, Train Loss: 0.0418, Val Loss: 7.3828\n",
      "Epoch: 311, Batch: 0, Loss: 7.1245\n",
      "Epoch: 311, Train Loss: 0.0417, Val Loss: 7.3784\n",
      "Epoch: 312, Batch: 0, Loss: 7.1984\n",
      "Epoch: 312, Train Loss: 0.0421, Val Loss: 7.3835\n",
      "Epoch: 313, Batch: 0, Loss: 7.4482\n",
      "Epoch: 313, Train Loss: 0.0436, Val Loss: 7.3794\n",
      "Epoch: 314, Batch: 0, Loss: 7.0895\n",
      "Epoch: 314, Train Loss: 0.0415, Val Loss: 7.3830\n",
      "Epoch: 315, Batch: 0, Loss: 7.4040\n",
      "Epoch: 315, Train Loss: 0.0433, Val Loss: 7.3811\n",
      "Epoch: 316, Batch: 0, Loss: 7.4791\n",
      "Epoch: 316, Train Loss: 0.0437, Val Loss: 7.3781\n",
      "Epoch: 317, Batch: 0, Loss: 7.5153\n",
      "Epoch: 317, Train Loss: 0.0439, Val Loss: 7.3768\n",
      "Epoch: 318, Batch: 0, Loss: 7.3226\n",
      "Epoch: 318, Train Loss: 0.0428, Val Loss: 7.3793\n",
      "Epoch: 319, Batch: 0, Loss: 7.3880\n",
      "Epoch: 319, Train Loss: 0.0432, Val Loss: 7.3816\n",
      "Epoch: 320, Batch: 0, Loss: 7.1164\n",
      "Epoch: 320, Train Loss: 0.0416, Val Loss: 7.3776\n",
      "Epoch: 321, Batch: 0, Loss: 7.2944\n",
      "Epoch: 321, Train Loss: 0.0427, Val Loss: 7.3739\n",
      "Epoch: 322, Batch: 0, Loss: 7.5190\n",
      "Epoch: 322, Train Loss: 0.0440, Val Loss: 7.3730\n",
      "Epoch: 323, Batch: 0, Loss: 7.3256\n",
      "Epoch: 323, Train Loss: 0.0428, Val Loss: 7.3743\n",
      "Epoch: 324, Batch: 0, Loss: 7.3577\n",
      "Epoch: 324, Train Loss: 0.0430, Val Loss: 7.3751\n",
      "Epoch: 325, Batch: 0, Loss: 7.2624\n",
      "Epoch: 325, Train Loss: 0.0425, Val Loss: 7.3744\n",
      "Epoch: 326, Batch: 0, Loss: 7.1689\n",
      "Epoch: 326, Train Loss: 0.0419, Val Loss: 7.3728\n",
      "Epoch: 327, Batch: 0, Loss: 7.4134\n",
      "Epoch: 327, Train Loss: 0.0434, Val Loss: 7.3696\n",
      "Epoch: 328, Batch: 0, Loss: 7.3743\n",
      "Epoch: 328, Train Loss: 0.0431, Val Loss: 7.3646\n",
      "Epoch: 329, Batch: 0, Loss: 7.3542\n",
      "Epoch: 329, Train Loss: 0.0430, Val Loss: 7.3625\n",
      "Epoch: 330, Batch: 0, Loss: 7.2666\n",
      "Epoch: 330, Train Loss: 0.0425, Val Loss: 7.3607\n",
      "Epoch: 331, Batch: 0, Loss: 7.2791\n",
      "Epoch: 331, Train Loss: 0.0426, Val Loss: 7.3604\n",
      "Epoch: 332, Batch: 0, Loss: 7.3583\n",
      "Epoch: 332, Train Loss: 0.0430, Val Loss: 7.3597\n",
      "Epoch: 333, Batch: 0, Loss: 7.4938\n",
      "Epoch: 333, Train Loss: 0.0438, Val Loss: 7.3586\n",
      "Epoch: 334, Batch: 0, Loss: 7.2943\n",
      "Epoch: 334, Train Loss: 0.0427, Val Loss: 7.3564\n",
      "Epoch: 335, Batch: 0, Loss: 7.2829\n",
      "Epoch: 335, Train Loss: 0.0426, Val Loss: 7.3540\n",
      "Epoch: 336, Batch: 0, Loss: 7.4404\n",
      "Epoch: 336, Train Loss: 0.0435, Val Loss: 7.3544\n",
      "Epoch: 337, Batch: 0, Loss: 7.2274\n",
      "Epoch: 337, Train Loss: 0.0423, Val Loss: 7.3540\n",
      "Epoch: 338, Batch: 0, Loss: 7.1981\n",
      "Epoch: 338, Train Loss: 0.0421, Val Loss: 7.3547\n",
      "Epoch: 339, Batch: 0, Loss: 7.2639\n",
      "Epoch: 339, Train Loss: 0.0425, Val Loss: 7.3555\n",
      "Epoch: 340, Batch: 0, Loss: 7.1911\n",
      "Epoch: 340, Train Loss: 0.0421, Val Loss: 7.3546\n",
      "Epoch: 341, Batch: 0, Loss: 7.1645\n",
      "Epoch: 341, Train Loss: 0.0419, Val Loss: 7.3544\n",
      "Epoch: 342, Batch: 0, Loss: 7.2257\n",
      "Epoch: 342, Train Loss: 0.0423, Val Loss: 7.3535\n",
      "Epoch: 343, Batch: 0, Loss: 7.3780\n",
      "Epoch: 343, Train Loss: 0.0431, Val Loss: 7.3517\n",
      "Epoch: 344, Batch: 0, Loss: 7.2894\n",
      "Epoch: 344, Train Loss: 0.0426, Val Loss: 7.3508\n",
      "Epoch: 345, Batch: 0, Loss: 7.1001\n",
      "Epoch: 345, Train Loss: 0.0415, Val Loss: 7.3466\n",
      "Epoch: 346, Batch: 0, Loss: 7.4385\n",
      "Epoch: 346, Train Loss: 0.0435, Val Loss: 7.3449\n",
      "Epoch: 347, Batch: 0, Loss: 7.2020\n",
      "Epoch: 347, Train Loss: 0.0421, Val Loss: 7.3430\n",
      "Epoch: 348, Batch: 0, Loss: 7.3633\n",
      "Epoch: 348, Train Loss: 0.0431, Val Loss: 7.3406\n",
      "Epoch: 349, Batch: 0, Loss: 7.1785\n",
      "Epoch: 349, Train Loss: 0.0420, Val Loss: 7.3416\n",
      "Epoch: 350, Batch: 0, Loss: 7.1641\n",
      "Epoch: 350, Train Loss: 0.0419, Val Loss: 7.3391\n",
      "Epoch: 351, Batch: 0, Loss: 7.5259\n",
      "Epoch: 351, Train Loss: 0.0440, Val Loss: 7.3376\n",
      "Epoch: 352, Batch: 0, Loss: 7.1179\n",
      "Epoch: 352, Train Loss: 0.0416, Val Loss: 7.3366\n",
      "Epoch: 353, Batch: 0, Loss: 7.2264\n",
      "Epoch: 353, Train Loss: 0.0423, Val Loss: 7.3337\n",
      "Epoch: 354, Batch: 0, Loss: 6.9601\n",
      "Epoch: 354, Train Loss: 0.0407, Val Loss: 7.3342\n",
      "Epoch: 355, Batch: 0, Loss: 7.2661\n",
      "Epoch: 355, Train Loss: 0.0425, Val Loss: 7.3306\n",
      "Epoch: 356, Batch: 0, Loss: 7.4410\n",
      "Epoch: 356, Train Loss: 0.0435, Val Loss: 7.3343\n",
      "Epoch: 357, Batch: 0, Loss: 7.1940\n",
      "Epoch: 357, Train Loss: 0.0421, Val Loss: 7.3306\n",
      "Epoch: 358, Batch: 0, Loss: 7.2593\n",
      "Epoch: 358, Train Loss: 0.0425, Val Loss: 7.3373\n",
      "Epoch: 359, Batch: 0, Loss: 7.4125\n",
      "Epoch: 359, Train Loss: 0.0433, Val Loss: 7.3266\n",
      "Epoch: 360, Batch: 0, Loss: 7.3343\n",
      "Epoch: 360, Train Loss: 0.0429, Val Loss: 7.3300\n",
      "Epoch: 361, Batch: 0, Loss: 7.4697\n",
      "Epoch: 361, Train Loss: 0.0437, Val Loss: 7.3265\n",
      "Epoch: 362, Batch: 0, Loss: 7.2214\n",
      "Epoch: 362, Train Loss: 0.0422, Val Loss: 7.3239\n",
      "Epoch: 363, Batch: 0, Loss: 7.2882\n",
      "Epoch: 363, Train Loss: 0.0426, Val Loss: 7.3181\n",
      "Epoch: 364, Batch: 0, Loss: 7.1153\n",
      "Epoch: 364, Train Loss: 0.0416, Val Loss: 7.3123\n",
      "Epoch: 365, Batch: 0, Loss: 7.2752\n",
      "Epoch: 365, Train Loss: 0.0425, Val Loss: 7.3147\n",
      "Epoch: 366, Batch: 0, Loss: 7.0503\n",
      "Epoch: 366, Train Loss: 0.0412, Val Loss: 7.3137\n",
      "Epoch: 367, Batch: 0, Loss: 7.2245\n",
      "Epoch: 367, Train Loss: 0.0422, Val Loss: 7.3116\n",
      "Epoch: 368, Batch: 0, Loss: 7.2245\n",
      "Epoch: 368, Train Loss: 0.0422, Val Loss: 7.3088\n",
      "Epoch: 369, Batch: 0, Loss: 7.4972\n",
      "Epoch: 369, Train Loss: 0.0438, Val Loss: 7.3047\n",
      "Epoch: 370, Batch: 0, Loss: 7.3858\n",
      "Epoch: 370, Train Loss: 0.0432, Val Loss: 7.2987\n",
      "Epoch: 371, Batch: 0, Loss: 7.1460\n",
      "Epoch: 371, Train Loss: 0.0418, Val Loss: 7.3048\n",
      "Epoch: 372, Batch: 0, Loss: 7.1240\n",
      "Epoch: 372, Train Loss: 0.0417, Val Loss: 7.3124\n",
      "Epoch: 373, Batch: 0, Loss: 7.3429\n",
      "Epoch: 373, Train Loss: 0.0429, Val Loss: 7.3185\n",
      "Epoch: 374, Batch: 0, Loss: 7.2335\n",
      "Epoch: 374, Train Loss: 0.0423, Val Loss: 7.3029\n",
      "Epoch: 375, Batch: 0, Loss: 7.3344\n",
      "Epoch: 375, Train Loss: 0.0429, Val Loss: 7.3143\n",
      "Epoch: 376, Batch: 0, Loss: 7.2206\n",
      "Epoch: 376, Train Loss: 0.0422, Val Loss: 7.2967\n",
      "Epoch: 377, Batch: 0, Loss: 6.9882\n",
      "Epoch: 377, Train Loss: 0.0409, Val Loss: 7.3007\n",
      "Epoch: 378, Batch: 0, Loss: 7.0879\n",
      "Epoch: 378, Train Loss: 0.0414, Val Loss: 7.2935\n",
      "Epoch: 379, Batch: 0, Loss: 7.1632\n",
      "Epoch: 379, Train Loss: 0.0419, Val Loss: 7.2985\n",
      "Epoch: 380, Batch: 0, Loss: 7.4997\n",
      "Epoch: 380, Train Loss: 0.0439, Val Loss: 7.2831\n",
      "Epoch: 381, Batch: 0, Loss: 7.2372\n",
      "Epoch: 381, Train Loss: 0.0423, Val Loss: 7.2855\n",
      "Epoch: 382, Batch: 0, Loss: 7.2651\n",
      "Epoch: 382, Train Loss: 0.0425, Val Loss: 7.2792\n",
      "Epoch: 383, Batch: 0, Loss: 7.3062\n",
      "Epoch: 383, Train Loss: 0.0427, Val Loss: 7.2790\n",
      "Epoch: 384, Batch: 0, Loss: 7.0718\n",
      "Epoch: 384, Train Loss: 0.0414, Val Loss: 7.2680\n",
      "Epoch: 385, Batch: 0, Loss: 7.2054\n",
      "Epoch: 385, Train Loss: 0.0421, Val Loss: 7.2664\n",
      "Epoch: 386, Batch: 0, Loss: 7.0678\n",
      "Epoch: 386, Train Loss: 0.0413, Val Loss: 7.2643\n",
      "Epoch: 387, Batch: 0, Loss: 7.1771\n",
      "Epoch: 387, Train Loss: 0.0420, Val Loss: 7.2568\n",
      "Epoch: 388, Batch: 0, Loss: 7.1984\n",
      "Epoch: 388, Train Loss: 0.0421, Val Loss: 7.2573\n",
      "Epoch: 389, Batch: 0, Loss: 7.1619\n",
      "Epoch: 389, Train Loss: 0.0419, Val Loss: 7.2572\n",
      "Epoch: 390, Batch: 0, Loss: 7.1548\n",
      "Epoch: 390, Train Loss: 0.0418, Val Loss: 7.2565\n",
      "Epoch: 391, Batch: 0, Loss: 7.2765\n",
      "Epoch: 391, Train Loss: 0.0426, Val Loss: 7.2517\n",
      "Epoch: 392, Batch: 0, Loss: 7.1334\n",
      "Epoch: 392, Train Loss: 0.0417, Val Loss: 7.2469\n",
      "Epoch: 393, Batch: 0, Loss: 7.2874\n",
      "Epoch: 393, Train Loss: 0.0426, Val Loss: 7.2508\n",
      "Epoch: 394, Batch: 0, Loss: 7.3625\n",
      "Epoch: 394, Train Loss: 0.0431, Val Loss: 7.2408\n",
      "Epoch: 395, Batch: 0, Loss: 7.2183\n",
      "Epoch: 395, Train Loss: 0.0422, Val Loss: 7.2452\n",
      "Epoch: 396, Batch: 0, Loss: 7.3253\n",
      "Epoch: 396, Train Loss: 0.0428, Val Loss: 7.2441\n",
      "Epoch: 397, Batch: 0, Loss: 7.1192\n",
      "Epoch: 397, Train Loss: 0.0416, Val Loss: 7.2419\n",
      "Epoch: 398, Batch: 0, Loss: 6.8944\n",
      "Epoch: 398, Train Loss: 0.0403, Val Loss: 7.2455\n",
      "Epoch: 399, Batch: 0, Loss: 7.2416\n",
      "Epoch: 399, Train Loss: 0.0423, Val Loss: 7.2372\n",
      "Epoch: 400, Batch: 0, Loss: 7.3295\n",
      "Epoch: 400, Train Loss: 0.0429, Val Loss: 7.2353\n",
      "Epoch: 401, Batch: 0, Loss: 7.0980\n",
      "Epoch: 401, Train Loss: 0.0415, Val Loss: 7.2328\n",
      "Epoch: 402, Batch: 0, Loss: 7.2473\n",
      "Epoch: 402, Train Loss: 0.0424, Val Loss: 7.2297\n",
      "Epoch: 403, Batch: 0, Loss: 7.0190\n",
      "Epoch: 403, Train Loss: 0.0410, Val Loss: 7.2237\n",
      "Epoch: 404, Batch: 0, Loss: 7.0282\n",
      "Epoch: 404, Train Loss: 0.0411, Val Loss: 7.2228\n",
      "Epoch: 405, Batch: 0, Loss: 7.5304\n",
      "Epoch: 405, Train Loss: 0.0440, Val Loss: 7.2215\n",
      "Epoch: 406, Batch: 0, Loss: 7.0746\n",
      "Epoch: 406, Train Loss: 0.0414, Val Loss: 7.2199\n",
      "Epoch: 407, Batch: 0, Loss: 7.2444\n",
      "Epoch: 407, Train Loss: 0.0424, Val Loss: 7.2217\n",
      "Epoch: 408, Batch: 0, Loss: 6.9951\n",
      "Epoch: 408, Train Loss: 0.0409, Val Loss: 7.2181\n",
      "Epoch: 409, Batch: 0, Loss: 7.1581\n",
      "Epoch: 409, Train Loss: 0.0419, Val Loss: 7.2136\n",
      "Epoch: 410, Batch: 0, Loss: 7.1032\n",
      "Epoch: 410, Train Loss: 0.0415, Val Loss: 7.2120\n",
      "Epoch: 411, Batch: 0, Loss: 7.1235\n",
      "Epoch: 411, Train Loss: 0.0417, Val Loss: 7.2113\n",
      "Epoch: 412, Batch: 0, Loss: 7.3232\n",
      "Epoch: 412, Train Loss: 0.0428, Val Loss: 7.2129\n",
      "Epoch: 413, Batch: 0, Loss: 7.1004\n",
      "Epoch: 413, Train Loss: 0.0415, Val Loss: 7.2063\n",
      "Epoch: 414, Batch: 0, Loss: 7.4480\n",
      "Epoch: 414, Train Loss: 0.0436, Val Loss: 7.2032\n",
      "Epoch: 415, Batch: 0, Loss: 7.1082\n",
      "Epoch: 415, Train Loss: 0.0416, Val Loss: 7.2019\n",
      "Epoch: 416, Batch: 0, Loss: 7.0034\n",
      "Epoch: 416, Train Loss: 0.0410, Val Loss: 7.1979\n",
      "Epoch: 417, Batch: 0, Loss: 7.1615\n",
      "Epoch: 417, Train Loss: 0.0419, Val Loss: 7.1962\n",
      "Epoch: 418, Batch: 0, Loss: 7.0629\n",
      "Epoch: 418, Train Loss: 0.0413, Val Loss: 7.1981\n",
      "Epoch: 419, Batch: 0, Loss: 7.2803\n",
      "Epoch: 419, Train Loss: 0.0426, Val Loss: 7.1990\n",
      "Epoch: 420, Batch: 0, Loss: 7.0528\n",
      "Epoch: 420, Train Loss: 0.0412, Val Loss: 7.1965\n",
      "Epoch: 421, Batch: 0, Loss: 7.0362\n",
      "Epoch: 421, Train Loss: 0.0411, Val Loss: 7.1939\n",
      "Epoch: 422, Batch: 0, Loss: 7.2203\n",
      "Epoch: 422, Train Loss: 0.0422, Val Loss: 7.1927\n",
      "Epoch: 423, Batch: 0, Loss: 7.0440\n",
      "Epoch: 423, Train Loss: 0.0412, Val Loss: 7.1937\n",
      "Epoch: 424, Batch: 0, Loss: 6.9413\n",
      "Epoch: 424, Train Loss: 0.0406, Val Loss: 7.1890\n",
      "Epoch: 425, Batch: 0, Loss: 7.5623\n",
      "Epoch: 425, Train Loss: 0.0442, Val Loss: 7.1913\n",
      "Epoch: 426, Batch: 0, Loss: 7.2717\n",
      "Epoch: 426, Train Loss: 0.0425, Val Loss: 7.1870\n",
      "Epoch: 427, Batch: 0, Loss: 6.9943\n",
      "Epoch: 427, Train Loss: 0.0409, Val Loss: 7.1865\n",
      "Epoch: 428, Batch: 0, Loss: 7.3160\n",
      "Epoch: 428, Train Loss: 0.0428, Val Loss: 7.1856\n",
      "Epoch: 429, Batch: 0, Loss: 7.0173\n",
      "Epoch: 429, Train Loss: 0.0410, Val Loss: 7.1826\n",
      "Epoch: 430, Batch: 0, Loss: 7.3002\n",
      "Epoch: 430, Train Loss: 0.0427, Val Loss: 7.1819\n",
      "Epoch: 431, Batch: 0, Loss: 6.9855\n",
      "Epoch: 431, Train Loss: 0.0409, Val Loss: 7.1813\n",
      "Epoch: 432, Batch: 0, Loss: 7.1470\n",
      "Epoch: 432, Train Loss: 0.0418, Val Loss: 7.1831\n",
      "Epoch: 433, Batch: 0, Loss: 7.3428\n",
      "Epoch: 433, Train Loss: 0.0429, Val Loss: 7.1798\n",
      "Epoch: 434, Batch: 0, Loss: 6.9429\n",
      "Epoch: 434, Train Loss: 0.0406, Val Loss: 7.1815\n",
      "Epoch: 435, Batch: 0, Loss: 7.2080\n",
      "Epoch: 435, Train Loss: 0.0422, Val Loss: 7.1799\n",
      "Epoch: 436, Batch: 0, Loss: 7.1622\n",
      "Epoch: 436, Train Loss: 0.0419, Val Loss: 7.1776\n",
      "Epoch: 437, Batch: 0, Loss: 7.1793\n",
      "Epoch: 437, Train Loss: 0.0420, Val Loss: 7.1759\n",
      "Epoch: 438, Batch: 0, Loss: 7.1383\n",
      "Epoch: 438, Train Loss: 0.0417, Val Loss: 7.1745\n",
      "Epoch: 439, Batch: 0, Loss: 6.9338\n",
      "Epoch: 439, Train Loss: 0.0405, Val Loss: 7.1722\n",
      "Epoch: 440, Batch: 0, Loss: 6.8393\n",
      "Epoch: 440, Train Loss: 0.0400, Val Loss: 7.1723\n",
      "Epoch: 441, Batch: 0, Loss: 7.1935\n",
      "Epoch: 441, Train Loss: 0.0421, Val Loss: 7.1715\n",
      "Epoch: 442, Batch: 0, Loss: 7.0277\n",
      "Epoch: 442, Train Loss: 0.0411, Val Loss: 7.1700\n",
      "Epoch: 443, Batch: 0, Loss: 7.1511\n",
      "Epoch: 443, Train Loss: 0.0418, Val Loss: 7.1678\n",
      "Epoch: 444, Batch: 0, Loss: 7.0465\n",
      "Epoch: 444, Train Loss: 0.0412, Val Loss: 7.1683\n",
      "Epoch: 445, Batch: 0, Loss: 7.1275\n",
      "Epoch: 445, Train Loss: 0.0417, Val Loss: 7.1675\n",
      "Epoch: 446, Batch: 0, Loss: 6.9425\n",
      "Epoch: 446, Train Loss: 0.0406, Val Loss: 7.1660\n",
      "Epoch: 447, Batch: 0, Loss: 7.3889\n",
      "Epoch: 447, Train Loss: 0.0432, Val Loss: 7.1652\n",
      "Epoch: 448, Batch: 0, Loss: 7.0811\n",
      "Epoch: 448, Train Loss: 0.0414, Val Loss: 7.1623\n",
      "Epoch: 449, Batch: 0, Loss: 7.0457\n",
      "Epoch: 449, Train Loss: 0.0412, Val Loss: 7.1608\n",
      "Epoch: 450, Batch: 0, Loss: 7.1178\n",
      "Epoch: 450, Train Loss: 0.0416, Val Loss: 7.1591\n",
      "Epoch: 451, Batch: 0, Loss: 6.8228\n",
      "Epoch: 451, Train Loss: 0.0399, Val Loss: 7.1557\n",
      "Epoch: 452, Batch: 0, Loss: 7.2309\n",
      "Epoch: 452, Train Loss: 0.0423, Val Loss: 7.1562\n",
      "Epoch: 453, Batch: 0, Loss: 7.3887\n",
      "Epoch: 453, Train Loss: 0.0432, Val Loss: 7.1564\n",
      "Epoch: 454, Batch: 0, Loss: 6.9797\n",
      "Epoch: 454, Train Loss: 0.0408, Val Loss: 7.1549\n",
      "Epoch: 455, Batch: 0, Loss: 7.2047\n",
      "Epoch: 455, Train Loss: 0.0421, Val Loss: 7.1534\n",
      "Epoch: 456, Batch: 0, Loss: 6.9043\n",
      "Epoch: 456, Train Loss: 0.0404, Val Loss: 7.1545\n",
      "Epoch: 457, Batch: 0, Loss: 6.9898\n",
      "Epoch: 457, Train Loss: 0.0409, Val Loss: 7.1530\n",
      "Epoch: 458, Batch: 0, Loss: 7.2729\n",
      "Epoch: 458, Train Loss: 0.0425, Val Loss: 7.1451\n",
      "Epoch: 459, Batch: 0, Loss: 6.8840\n",
      "Epoch: 459, Train Loss: 0.0403, Val Loss: 7.1433\n",
      "Epoch: 460, Batch: 0, Loss: 7.0639\n",
      "Epoch: 460, Train Loss: 0.0413, Val Loss: 7.1417\n",
      "Epoch: 461, Batch: 0, Loss: 7.1228\n",
      "Epoch: 461, Train Loss: 0.0417, Val Loss: 7.1385\n",
      "Epoch: 462, Batch: 0, Loss: 6.9109\n",
      "Epoch: 462, Train Loss: 0.0404, Val Loss: 7.1379\n",
      "Epoch: 463, Batch: 0, Loss: 6.8538\n",
      "Epoch: 463, Train Loss: 0.0401, Val Loss: 7.1372\n",
      "Epoch: 464, Batch: 0, Loss: 7.1656\n",
      "Epoch: 464, Train Loss: 0.0419, Val Loss: 7.1346\n",
      "Epoch: 465, Batch: 0, Loss: 7.1273\n",
      "Epoch: 465, Train Loss: 0.0417, Val Loss: 7.1336\n",
      "Epoch: 466, Batch: 0, Loss: 7.1970\n",
      "Epoch: 466, Train Loss: 0.0421, Val Loss: 7.1340\n",
      "Epoch: 467, Batch: 0, Loss: 7.0145\n",
      "Epoch: 467, Train Loss: 0.0410, Val Loss: 7.1333\n",
      "Epoch: 468, Batch: 0, Loss: 7.0392\n",
      "Epoch: 468, Train Loss: 0.0412, Val Loss: 7.1293\n",
      "Epoch: 469, Batch: 0, Loss: 7.2490\n",
      "Epoch: 469, Train Loss: 0.0424, Val Loss: 7.1283\n",
      "Epoch: 470, Batch: 0, Loss: 7.1316\n",
      "Epoch: 470, Train Loss: 0.0417, Val Loss: 7.1290\n",
      "Epoch: 471, Batch: 0, Loss: 7.0071\n",
      "Epoch: 471, Train Loss: 0.0410, Val Loss: 7.1246\n",
      "Epoch: 472, Batch: 0, Loss: 6.8612\n",
      "Epoch: 472, Train Loss: 0.0401, Val Loss: 7.1238\n",
      "Epoch: 473, Batch: 0, Loss: 7.0130\n",
      "Epoch: 473, Train Loss: 0.0410, Val Loss: 7.1235\n",
      "Epoch: 474, Batch: 0, Loss: 7.1498\n",
      "Epoch: 474, Train Loss: 0.0418, Val Loss: 7.1208\n",
      "Epoch: 475, Batch: 0, Loss: 6.9454\n",
      "Epoch: 475, Train Loss: 0.0406, Val Loss: 7.1229\n",
      "Epoch: 476, Batch: 0, Loss: 7.0143\n",
      "Epoch: 476, Train Loss: 0.0410, Val Loss: 7.1191\n",
      "Epoch: 477, Batch: 0, Loss: 6.9496\n",
      "Epoch: 477, Train Loss: 0.0406, Val Loss: 7.1177\n",
      "Epoch: 478, Batch: 0, Loss: 7.0940\n",
      "Epoch: 478, Train Loss: 0.0415, Val Loss: 7.1165\n",
      "Epoch: 479, Batch: 0, Loss: 7.0640\n",
      "Epoch: 479, Train Loss: 0.0413, Val Loss: 7.1136\n",
      "Epoch: 480, Batch: 0, Loss: 7.0304\n",
      "Epoch: 480, Train Loss: 0.0411, Val Loss: 7.1093\n",
      "Epoch: 481, Batch: 0, Loss: 6.6940\n",
      "Epoch: 481, Train Loss: 0.0391, Val Loss: 7.1058\n",
      "Epoch: 482, Batch: 0, Loss: 6.8973\n",
      "Epoch: 482, Train Loss: 0.0403, Val Loss: 7.1092\n",
      "Epoch: 483, Batch: 0, Loss: 7.2483\n",
      "Epoch: 483, Train Loss: 0.0424, Val Loss: 7.1065\n",
      "Epoch: 484, Batch: 0, Loss: 6.9026\n",
      "Epoch: 484, Train Loss: 0.0404, Val Loss: 7.0970\n",
      "Epoch: 485, Batch: 0, Loss: 7.1773\n",
      "Epoch: 485, Train Loss: 0.0420, Val Loss: 7.0963\n",
      "Epoch: 486, Batch: 0, Loss: 6.8714\n",
      "Epoch: 486, Train Loss: 0.0402, Val Loss: 7.1014\n",
      "Epoch: 487, Batch: 0, Loss: 6.9040\n",
      "Epoch: 487, Train Loss: 0.0404, Val Loss: 7.0945\n",
      "Epoch: 488, Batch: 0, Loss: 6.7755\n",
      "Epoch: 488, Train Loss: 0.0396, Val Loss: 7.0999\n",
      "Epoch: 489, Batch: 0, Loss: 6.9078\n",
      "Epoch: 489, Train Loss: 0.0404, Val Loss: 7.0885\n",
      "Epoch: 490, Batch: 0, Loss: 7.0211\n",
      "Epoch: 490, Train Loss: 0.0411, Val Loss: 7.0898\n",
      "Epoch: 491, Batch: 0, Loss: 6.9900\n",
      "Epoch: 491, Train Loss: 0.0409, Val Loss: 7.0814\n",
      "Epoch: 492, Batch: 0, Loss: 6.9908\n",
      "Epoch: 492, Train Loss: 0.0409, Val Loss: 7.0721\n",
      "Epoch: 493, Batch: 0, Loss: 7.0000\n",
      "Epoch: 493, Train Loss: 0.0409, Val Loss: 7.0778\n",
      "Epoch: 494, Batch: 0, Loss: 7.0071\n",
      "Epoch: 494, Train Loss: 0.0410, Val Loss: 7.0681\n",
      "Epoch: 495, Batch: 0, Loss: 6.7651\n",
      "Epoch: 495, Train Loss: 0.0396, Val Loss: 7.0694\n",
      "Epoch: 496, Batch: 0, Loss: 6.7677\n",
      "Epoch: 496, Train Loss: 0.0396, Val Loss: 7.0606\n",
      "Epoch: 497, Batch: 0, Loss: 6.9071\n",
      "Epoch: 497, Train Loss: 0.0404, Val Loss: 7.0541\n",
      "Epoch: 498, Batch: 0, Loss: 6.7634\n",
      "Epoch: 498, Train Loss: 0.0396, Val Loss: 7.0551\n",
      "Epoch: 499, Batch: 0, Loss: 7.0655\n",
      "Epoch: 499, Train Loss: 0.0413, Val Loss: 7.0659\n",
      "Epoch: 500, Batch: 0, Loss: 7.1602\n",
      "Epoch: 500, Train Loss: 0.0419, Val Loss: 7.0536\n",
      "Epoch: 501, Batch: 0, Loss: 6.8355\n",
      "Epoch: 501, Train Loss: 0.0400, Val Loss: 7.0789\n",
      "Epoch: 502, Batch: 0, Loss: 6.9576\n",
      "Epoch: 502, Train Loss: 0.0407, Val Loss: 7.0626\n",
      "Epoch: 503, Batch: 0, Loss: 7.1032\n",
      "Epoch: 503, Train Loss: 0.0415, Val Loss: 7.0651\n",
      "Epoch: 504, Batch: 0, Loss: 6.8134\n",
      "Epoch: 504, Train Loss: 0.0398, Val Loss: 7.0560\n",
      "Epoch: 505, Batch: 0, Loss: 6.9876\n",
      "Epoch: 505, Train Loss: 0.0409, Val Loss: 7.0466\n",
      "Epoch: 506, Batch: 0, Loss: 6.7702\n",
      "Epoch: 506, Train Loss: 0.0396, Val Loss: 7.0596\n",
      "Epoch: 507, Batch: 0, Loss: 7.0162\n",
      "Epoch: 507, Train Loss: 0.0410, Val Loss: 7.0558\n",
      "Epoch: 508, Batch: 0, Loss: 6.9229\n",
      "Epoch: 508, Train Loss: 0.0405, Val Loss: 7.0617\n",
      "Epoch: 509, Batch: 0, Loss: 6.9212\n",
      "Epoch: 509, Train Loss: 0.0405, Val Loss: 7.0319\n",
      "Epoch: 510, Batch: 0, Loss: 7.1082\n",
      "Epoch: 510, Train Loss: 0.0416, Val Loss: 7.0411\n",
      "Epoch: 511, Batch: 0, Loss: 7.0132\n",
      "Epoch: 511, Train Loss: 0.0410, Val Loss: 7.0349\n",
      "Epoch: 512, Batch: 0, Loss: 6.8561\n",
      "Epoch: 512, Train Loss: 0.0401, Val Loss: 7.0277\n",
      "Epoch: 513, Batch: 0, Loss: 7.0834\n",
      "Epoch: 513, Train Loss: 0.0414, Val Loss: 7.0202\n",
      "Epoch: 514, Batch: 0, Loss: 6.8207\n",
      "Epoch: 514, Train Loss: 0.0399, Val Loss: 7.0261\n",
      "Epoch: 515, Batch: 0, Loss: 6.8357\n",
      "Epoch: 515, Train Loss: 0.0400, Val Loss: 7.0234\n",
      "Epoch: 516, Batch: 0, Loss: 6.8525\n",
      "Epoch: 516, Train Loss: 0.0401, Val Loss: 7.0241\n",
      "Epoch: 517, Batch: 0, Loss: 6.7886\n",
      "Epoch: 517, Train Loss: 0.0397, Val Loss: 7.0161\n",
      "Epoch: 518, Batch: 0, Loss: 6.9129\n",
      "Epoch: 518, Train Loss: 0.0404, Val Loss: 7.0176\n",
      "Epoch: 519, Batch: 0, Loss: 6.6180\n",
      "Epoch: 519, Train Loss: 0.0387, Val Loss: 7.0243\n",
      "Epoch: 520, Batch: 0, Loss: 6.8628\n",
      "Epoch: 520, Train Loss: 0.0401, Val Loss: 7.0105\n",
      "Epoch: 521, Batch: 0, Loss: 7.0202\n",
      "Epoch: 521, Train Loss: 0.0411, Val Loss: 7.0200\n",
      "Epoch: 522, Batch: 0, Loss: 6.9113\n",
      "Epoch: 522, Train Loss: 0.0404, Val Loss: 7.0119\n",
      "Epoch: 523, Batch: 0, Loss: 7.0291\n",
      "Epoch: 523, Train Loss: 0.0411, Val Loss: 7.0150\n",
      "Epoch: 524, Batch: 0, Loss: 7.2428\n",
      "Epoch: 524, Train Loss: 0.0424, Val Loss: 7.0085\n",
      "Epoch: 525, Batch: 0, Loss: 6.7075\n",
      "Epoch: 525, Train Loss: 0.0392, Val Loss: 7.0117\n",
      "Epoch: 526, Batch: 0, Loss: 7.0002\n",
      "Epoch: 526, Train Loss: 0.0409, Val Loss: 7.0000\n",
      "Epoch: 527, Batch: 0, Loss: 7.1962\n",
      "Epoch: 527, Train Loss: 0.0421, Val Loss: 6.9951\n",
      "Epoch: 528, Batch: 0, Loss: 6.9521\n",
      "Epoch: 528, Train Loss: 0.0407, Val Loss: 6.9882\n",
      "Epoch: 529, Batch: 0, Loss: 6.9560\n",
      "Epoch: 529, Train Loss: 0.0407, Val Loss: 6.9879\n",
      "Epoch: 530, Batch: 0, Loss: 6.9976\n",
      "Epoch: 530, Train Loss: 0.0409, Val Loss: 6.9876\n",
      "Epoch: 531, Batch: 0, Loss: 6.9878\n",
      "Epoch: 531, Train Loss: 0.0409, Val Loss: 6.9870\n",
      "Epoch: 532, Batch: 0, Loss: 7.0286\n",
      "Epoch: 532, Train Loss: 0.0411, Val Loss: 6.9851\n",
      "Epoch: 533, Batch: 0, Loss: 6.9459\n",
      "Epoch: 533, Train Loss: 0.0406, Val Loss: 6.9832\n",
      "Epoch: 534, Batch: 0, Loss: 6.8844\n",
      "Epoch: 534, Train Loss: 0.0403, Val Loss: 6.9835\n",
      "Epoch: 535, Batch: 0, Loss: 6.8801\n",
      "Epoch: 535, Train Loss: 0.0402, Val Loss: 6.9844\n",
      "Epoch: 536, Batch: 0, Loss: 6.7540\n",
      "Epoch: 536, Train Loss: 0.0395, Val Loss: 6.9800\n",
      "Epoch: 537, Batch: 0, Loss: 6.7696\n",
      "Epoch: 537, Train Loss: 0.0396, Val Loss: 6.9721\n",
      "Epoch: 538, Batch: 0, Loss: 6.9527\n",
      "Epoch: 538, Train Loss: 0.0407, Val Loss: 6.9697\n",
      "Epoch: 539, Batch: 0, Loss: 6.7144\n",
      "Epoch: 539, Train Loss: 0.0393, Val Loss: 6.9704\n",
      "Epoch: 540, Batch: 0, Loss: 6.7758\n",
      "Epoch: 540, Train Loss: 0.0396, Val Loss: 6.9649\n",
      "Epoch: 541, Batch: 0, Loss: 6.8506\n",
      "Epoch: 541, Train Loss: 0.0401, Val Loss: 6.9588\n",
      "Epoch: 542, Batch: 0, Loss: 7.1816\n",
      "Epoch: 542, Train Loss: 0.0420, Val Loss: 6.9572\n",
      "Epoch: 543, Batch: 0, Loss: 6.9165\n",
      "Epoch: 543, Train Loss: 0.0404, Val Loss: 6.9571\n",
      "Epoch: 544, Batch: 0, Loss: 6.8690\n",
      "Epoch: 544, Train Loss: 0.0402, Val Loss: 6.9578\n",
      "Epoch: 545, Batch: 0, Loss: 7.0469\n",
      "Epoch: 545, Train Loss: 0.0412, Val Loss: 6.9562\n",
      "Epoch: 546, Batch: 0, Loss: 6.7181\n",
      "Epoch: 546, Train Loss: 0.0393, Val Loss: 6.9527\n",
      "Epoch: 547, Batch: 0, Loss: 6.7015\n",
      "Epoch: 547, Train Loss: 0.0392, Val Loss: 6.9499\n",
      "Epoch: 548, Batch: 0, Loss: 7.0756\n",
      "Epoch: 548, Train Loss: 0.0414, Val Loss: 6.9512\n",
      "Epoch: 549, Batch: 0, Loss: 6.7352\n",
      "Epoch: 549, Train Loss: 0.0394, Val Loss: 6.9518\n",
      "Epoch: 550, Batch: 0, Loss: 6.8546\n",
      "Epoch: 550, Train Loss: 0.0401, Val Loss: 6.9507\n",
      "Epoch: 551, Batch: 0, Loss: 7.0858\n",
      "Epoch: 551, Train Loss: 0.0414, Val Loss: 6.9425\n",
      "Epoch: 552, Batch: 0, Loss: 6.6935\n",
      "Epoch: 552, Train Loss: 0.0391, Val Loss: 6.9455\n",
      "Epoch: 553, Batch: 0, Loss: 6.9495\n",
      "Epoch: 553, Train Loss: 0.0406, Val Loss: 6.9452\n",
      "Epoch: 554, Batch: 0, Loss: 6.9314\n",
      "Epoch: 554, Train Loss: 0.0405, Val Loss: 6.9491\n",
      "Epoch: 555, Batch: 0, Loss: 6.9449\n",
      "Epoch: 555, Train Loss: 0.0406, Val Loss: 6.9492\n",
      "Epoch: 556, Batch: 0, Loss: 6.4813\n",
      "Epoch: 556, Train Loss: 0.0379, Val Loss: 6.9527\n",
      "Epoch: 557, Batch: 0, Loss: 6.6945\n",
      "Epoch: 557, Train Loss: 0.0391, Val Loss: 6.9396\n",
      "Epoch: 558, Batch: 0, Loss: 6.7510\n",
      "Epoch: 558, Train Loss: 0.0395, Val Loss: 6.9420\n",
      "Epoch: 559, Batch: 0, Loss: 6.5372\n",
      "Epoch: 559, Train Loss: 0.0382, Val Loss: 6.9401\n",
      "Epoch: 560, Batch: 0, Loss: 6.7399\n",
      "Epoch: 560, Train Loss: 0.0394, Val Loss: 6.9437\n",
      "Epoch: 561, Batch: 0, Loss: 6.8230\n",
      "Epoch: 561, Train Loss: 0.0399, Val Loss: 6.9433\n",
      "Epoch: 562, Batch: 0, Loss: 6.2285\n",
      "Epoch: 562, Train Loss: 0.0364, Val Loss: 6.9416\n",
      "Epoch: 563, Batch: 0, Loss: 6.8211\n",
      "Epoch: 563, Train Loss: 0.0399, Val Loss: 6.9375\n",
      "Epoch: 564, Batch: 0, Loss: 6.9484\n",
      "Epoch: 564, Train Loss: 0.0406, Val Loss: 6.9331\n",
      "Epoch: 565, Batch: 0, Loss: 6.6651\n",
      "Epoch: 565, Train Loss: 0.0390, Val Loss: 6.9330\n",
      "Epoch: 566, Batch: 0, Loss: 6.8767\n",
      "Epoch: 566, Train Loss: 0.0402, Val Loss: 6.9293\n",
      "Epoch: 567, Batch: 0, Loss: 6.6866\n",
      "Epoch: 567, Train Loss: 0.0391, Val Loss: 6.9278\n",
      "Epoch: 568, Batch: 0, Loss: 6.7145\n",
      "Epoch: 568, Train Loss: 0.0393, Val Loss: 6.9290\n",
      "Epoch: 569, Batch: 0, Loss: 6.3533\n",
      "Epoch: 569, Train Loss: 0.0372, Val Loss: 6.9266\n",
      "Epoch: 570, Batch: 0, Loss: 7.0194\n",
      "Epoch: 570, Train Loss: 0.0410, Val Loss: 6.9244\n",
      "Epoch: 571, Batch: 0, Loss: 6.8597\n",
      "Epoch: 571, Train Loss: 0.0401, Val Loss: 6.9209\n",
      "Epoch: 572, Batch: 0, Loss: 6.9241\n",
      "Epoch: 572, Train Loss: 0.0405, Val Loss: 6.9216\n",
      "Epoch: 573, Batch: 0, Loss: 6.9091\n",
      "Epoch: 573, Train Loss: 0.0404, Val Loss: 6.9161\n",
      "Epoch: 574, Batch: 0, Loss: 6.6994\n",
      "Epoch: 574, Train Loss: 0.0392, Val Loss: 6.9107\n",
      "Epoch: 575, Batch: 0, Loss: 6.7386\n",
      "Epoch: 575, Train Loss: 0.0394, Val Loss: 6.9077\n",
      "Epoch: 576, Batch: 0, Loss: 6.9161\n",
      "Epoch: 576, Train Loss: 0.0404, Val Loss: 6.9068\n",
      "Epoch: 577, Batch: 0, Loss: 6.8015\n",
      "Epoch: 577, Train Loss: 0.0398, Val Loss: 6.9080\n",
      "Epoch: 578, Batch: 0, Loss: 6.6564\n",
      "Epoch: 578, Train Loss: 0.0389, Val Loss: 6.9109\n",
      "Epoch: 579, Batch: 0, Loss: 6.8834\n",
      "Epoch: 579, Train Loss: 0.0403, Val Loss: 6.9102\n",
      "Epoch: 580, Batch: 0, Loss: 6.6730\n",
      "Epoch: 580, Train Loss: 0.0390, Val Loss: 6.9054\n",
      "Epoch: 581, Batch: 0, Loss: 6.9308\n",
      "Epoch: 581, Train Loss: 0.0405, Val Loss: 6.9040\n",
      "Epoch: 582, Batch: 0, Loss: 6.7734\n",
      "Epoch: 582, Train Loss: 0.0396, Val Loss: 6.9049\n",
      "Epoch: 583, Batch: 0, Loss: 6.9937\n",
      "Epoch: 583, Train Loss: 0.0409, Val Loss: 6.9011\n",
      "Epoch: 584, Batch: 0, Loss: 7.0865\n",
      "Epoch: 584, Train Loss: 0.0414, Val Loss: 6.9056\n",
      "Epoch: 585, Batch: 0, Loss: 6.7443\n",
      "Epoch: 585, Train Loss: 0.0394, Val Loss: 6.9030\n",
      "Epoch: 586, Batch: 0, Loss: 6.9553\n",
      "Epoch: 586, Train Loss: 0.0407, Val Loss: 6.8997\n",
      "Epoch: 587, Batch: 0, Loss: 6.8690\n",
      "Epoch: 587, Train Loss: 0.0402, Val Loss: 6.8986\n",
      "Epoch: 588, Batch: 0, Loss: 6.6147\n",
      "Epoch: 588, Train Loss: 0.0387, Val Loss: 6.9000\n",
      "Epoch: 589, Batch: 0, Loss: 6.4482\n",
      "Epoch: 589, Train Loss: 0.0377, Val Loss: 6.9029\n",
      "Epoch: 590, Batch: 0, Loss: 6.8562\n",
      "Epoch: 590, Train Loss: 0.0401, Val Loss: 6.9013\n",
      "Epoch: 591, Batch: 0, Loss: 6.6079\n",
      "Epoch: 591, Train Loss: 0.0386, Val Loss: 6.9024\n",
      "Epoch: 592, Batch: 0, Loss: 6.5788\n",
      "Epoch: 592, Train Loss: 0.0385, Val Loss: 6.9038\n",
      "Epoch: 593, Batch: 0, Loss: 6.7829\n",
      "Epoch: 593, Train Loss: 0.0397, Val Loss: 6.8961\n",
      "Epoch: 594, Batch: 0, Loss: 6.9731\n",
      "Epoch: 594, Train Loss: 0.0408, Val Loss: 6.8919\n",
      "Epoch: 595, Batch: 0, Loss: 6.7696\n",
      "Epoch: 595, Train Loss: 0.0396, Val Loss: 6.8889\n",
      "Epoch: 596, Batch: 0, Loss: 6.5331\n",
      "Epoch: 596, Train Loss: 0.0382, Val Loss: 6.8882\n",
      "Epoch: 597, Batch: 0, Loss: 6.9089\n",
      "Epoch: 597, Train Loss: 0.0404, Val Loss: 6.8863\n",
      "Epoch: 598, Batch: 0, Loss: 6.7525\n",
      "Epoch: 598, Train Loss: 0.0395, Val Loss: 6.8876\n",
      "Epoch: 599, Batch: 0, Loss: 6.6625\n",
      "Epoch: 599, Train Loss: 0.0390, Val Loss: 6.8888\n",
      "Epoch: 600, Batch: 0, Loss: 6.5965\n",
      "Epoch: 600, Train Loss: 0.0386, Val Loss: 6.8943\n",
      "Epoch: 601, Batch: 0, Loss: 6.7840\n",
      "Epoch: 601, Train Loss: 0.0397, Val Loss: 6.8850\n",
      "Epoch: 602, Batch: 0, Loss: 6.5862\n",
      "Epoch: 602, Train Loss: 0.0385, Val Loss: 6.8795\n",
      "Epoch: 603, Batch: 0, Loss: 6.7569\n",
      "Epoch: 603, Train Loss: 0.0395, Val Loss: 6.8764\n",
      "Epoch: 604, Batch: 0, Loss: 6.6740\n",
      "Epoch: 604, Train Loss: 0.0390, Val Loss: 6.8838\n",
      "Epoch: 605, Batch: 0, Loss: 6.6377\n",
      "Epoch: 605, Train Loss: 0.0388, Val Loss: 6.8879\n",
      "Epoch: 606, Batch: 0, Loss: 6.8530\n",
      "Epoch: 606, Train Loss: 0.0401, Val Loss: 6.8874\n",
      "Epoch: 607, Batch: 0, Loss: 6.9449\n",
      "Epoch: 607, Train Loss: 0.0406, Val Loss: 6.8816\n",
      "Epoch: 608, Batch: 0, Loss: 6.9664\n",
      "Epoch: 608, Train Loss: 0.0407, Val Loss: 6.8933\n",
      "Epoch: 609, Batch: 0, Loss: 6.8890\n",
      "Epoch: 609, Train Loss: 0.0403, Val Loss: 6.8793\n",
      "Epoch: 610, Batch: 0, Loss: 6.6752\n",
      "Epoch: 610, Train Loss: 0.0390, Val Loss: 6.8938\n",
      "Epoch: 611, Batch: 0, Loss: 6.7903\n",
      "Epoch: 611, Train Loss: 0.0397, Val Loss: 6.8753\n",
      "Epoch: 612, Batch: 0, Loss: 6.8528\n",
      "Epoch: 612, Train Loss: 0.0401, Val Loss: 6.8954\n",
      "Epoch: 613, Batch: 0, Loss: 6.6379\n",
      "Epoch: 613, Train Loss: 0.0388, Val Loss: 6.8956\n",
      "Epoch: 614, Batch: 0, Loss: 6.5170\n",
      "Epoch: 614, Train Loss: 0.0381, Val Loss: 6.8995\n",
      "Epoch: 615, Batch: 0, Loss: 6.7507\n",
      "Epoch: 615, Train Loss: 0.0395, Val Loss: 6.8842\n",
      "Epoch: 616, Batch: 0, Loss: 7.1159\n",
      "Epoch: 616, Train Loss: 0.0416, Val Loss: 6.8808\n",
      "Epoch: 617, Batch: 0, Loss: 6.6837\n",
      "Epoch: 617, Train Loss: 0.0391, Val Loss: 6.8728\n",
      "Epoch: 618, Batch: 0, Loss: 6.7470\n",
      "Epoch: 618, Train Loss: 0.0395, Val Loss: 6.8701\n",
      "Epoch: 619, Batch: 0, Loss: 7.1685\n",
      "Epoch: 619, Train Loss: 0.0419, Val Loss: 6.8652\n",
      "Epoch: 620, Batch: 0, Loss: 6.5390\n",
      "Epoch: 620, Train Loss: 0.0382, Val Loss: 6.8706\n",
      "Epoch: 621, Batch: 0, Loss: 6.9815\n",
      "Epoch: 621, Train Loss: 0.0408, Val Loss: 6.8638\n",
      "Epoch: 622, Batch: 0, Loss: 6.8759\n",
      "Epoch: 622, Train Loss: 0.0402, Val Loss: 6.8552\n",
      "Epoch: 623, Batch: 0, Loss: 6.8219\n",
      "Epoch: 623, Train Loss: 0.0399, Val Loss: 6.8554\n",
      "Epoch: 624, Batch: 0, Loss: 6.7984\n",
      "Epoch: 624, Train Loss: 0.0398, Val Loss: 6.8556\n",
      "Epoch: 625, Batch: 0, Loss: 6.7755\n",
      "Epoch: 625, Train Loss: 0.0396, Val Loss: 6.8559\n",
      "Epoch: 626, Batch: 0, Loss: 6.6274\n",
      "Epoch: 626, Train Loss: 0.0388, Val Loss: 6.8541\n",
      "Epoch: 627, Batch: 0, Loss: 6.5895\n",
      "Epoch: 627, Train Loss: 0.0385, Val Loss: 6.8525\n",
      "Epoch: 628, Batch: 0, Loss: 6.9859\n",
      "Epoch: 628, Train Loss: 0.0409, Val Loss: 6.8511\n",
      "Epoch: 629, Batch: 0, Loss: 6.7930\n",
      "Epoch: 629, Train Loss: 0.0397, Val Loss: 6.8543\n",
      "Epoch: 630, Batch: 0, Loss: 6.8528\n",
      "Epoch: 630, Train Loss: 0.0401, Val Loss: 6.8557\n",
      "Epoch: 631, Batch: 0, Loss: 6.4023\n",
      "Epoch: 631, Train Loss: 0.0374, Val Loss: 6.8544\n",
      "Epoch: 632, Batch: 0, Loss: 6.8123\n",
      "Epoch: 632, Train Loss: 0.0398, Val Loss: 6.8493\n",
      "Epoch: 633, Batch: 0, Loss: 6.3265\n",
      "Epoch: 633, Train Loss: 0.0370, Val Loss: 6.8454\n",
      "Epoch: 634, Batch: 0, Loss: 7.0047\n",
      "Epoch: 634, Train Loss: 0.0410, Val Loss: 6.8474\n",
      "Epoch: 635, Batch: 0, Loss: 6.7455\n",
      "Epoch: 635, Train Loss: 0.0394, Val Loss: 6.8433\n",
      "Epoch: 636, Batch: 0, Loss: 6.5995\n",
      "Epoch: 636, Train Loss: 0.0386, Val Loss: 6.8443\n",
      "Epoch: 637, Batch: 0, Loss: 6.4389\n",
      "Epoch: 637, Train Loss: 0.0377, Val Loss: 6.8480\n",
      "Epoch: 638, Batch: 0, Loss: 6.5289\n",
      "Epoch: 638, Train Loss: 0.0382, Val Loss: 6.8541\n",
      "Epoch: 639, Batch: 0, Loss: 6.8234\n",
      "Epoch: 639, Train Loss: 0.0399, Val Loss: 6.8491\n",
      "Epoch: 640, Batch: 0, Loss: 6.5680\n",
      "Epoch: 640, Train Loss: 0.0384, Val Loss: 6.8443\n",
      "Epoch: 641, Batch: 0, Loss: 6.9003\n",
      "Epoch: 641, Train Loss: 0.0404, Val Loss: 6.8377\n",
      "Epoch: 642, Batch: 0, Loss: 6.5879\n",
      "Epoch: 642, Train Loss: 0.0385, Val Loss: 6.8427\n",
      "Epoch: 643, Batch: 0, Loss: 6.8079\n",
      "Epoch: 643, Train Loss: 0.0398, Val Loss: 6.8439\n",
      "Epoch: 644, Batch: 0, Loss: 6.5292\n",
      "Epoch: 644, Train Loss: 0.0382, Val Loss: 6.8425\n",
      "Epoch: 645, Batch: 0, Loss: 6.7292\n",
      "Epoch: 645, Train Loss: 0.0394, Val Loss: 6.8360\n",
      "Epoch: 646, Batch: 0, Loss: 6.7576\n",
      "Epoch: 646, Train Loss: 0.0395, Val Loss: 6.8392\n",
      "Epoch: 647, Batch: 0, Loss: 6.8969\n",
      "Epoch: 647, Train Loss: 0.0403, Val Loss: 6.8324\n",
      "Epoch: 648, Batch: 0, Loss: 6.8278\n",
      "Epoch: 648, Train Loss: 0.0399, Val Loss: 6.8306\n",
      "Epoch: 649, Batch: 0, Loss: 6.3990\n",
      "Epoch: 649, Train Loss: 0.0374, Val Loss: 6.8248\n",
      "Epoch: 650, Batch: 0, Loss: 6.8301\n",
      "Epoch: 650, Train Loss: 0.0399, Val Loss: 6.8322\n",
      "Epoch: 651, Batch: 0, Loss: 6.5949\n",
      "Epoch: 651, Train Loss: 0.0386, Val Loss: 6.8305\n",
      "Epoch: 652, Batch: 0, Loss: 6.6233\n",
      "Epoch: 652, Train Loss: 0.0387, Val Loss: 6.8236\n",
      "Epoch: 653, Batch: 0, Loss: 6.6703\n",
      "Epoch: 653, Train Loss: 0.0390, Val Loss: 6.8240\n",
      "Epoch: 654, Batch: 0, Loss: 6.7475\n",
      "Epoch: 654, Train Loss: 0.0395, Val Loss: 6.8234\n",
      "Epoch: 655, Batch: 0, Loss: 6.8344\n",
      "Epoch: 655, Train Loss: 0.0400, Val Loss: 6.8278\n",
      "Epoch: 656, Batch: 0, Loss: 6.6308\n",
      "Epoch: 656, Train Loss: 0.0388, Val Loss: 6.8220\n",
      "Epoch: 657, Batch: 0, Loss: 6.7014\n",
      "Epoch: 657, Train Loss: 0.0392, Val Loss: 6.8193\n",
      "Epoch: 658, Batch: 0, Loss: 6.6104\n",
      "Epoch: 658, Train Loss: 0.0387, Val Loss: 6.8181\n",
      "Epoch: 659, Batch: 0, Loss: 6.5843\n",
      "Epoch: 659, Train Loss: 0.0385, Val Loss: 6.8225\n",
      "Epoch: 660, Batch: 0, Loss: 6.4941\n",
      "Epoch: 660, Train Loss: 0.0380, Val Loss: 6.8169\n",
      "Epoch: 661, Batch: 0, Loss: 6.8719\n",
      "Epoch: 661, Train Loss: 0.0402, Val Loss: 6.8175\n",
      "Epoch: 662, Batch: 0, Loss: 6.7822\n",
      "Epoch: 662, Train Loss: 0.0397, Val Loss: 6.8129\n",
      "Epoch: 663, Batch: 0, Loss: 6.5874\n",
      "Epoch: 663, Train Loss: 0.0385, Val Loss: 6.8252\n",
      "Epoch: 664, Batch: 0, Loss: 6.6284\n",
      "Epoch: 664, Train Loss: 0.0388, Val Loss: 6.8149\n",
      "Epoch: 665, Batch: 0, Loss: 6.5210\n",
      "Epoch: 665, Train Loss: 0.0381, Val Loss: 6.8110\n",
      "Epoch: 666, Batch: 0, Loss: 6.6393\n",
      "Epoch: 666, Train Loss: 0.0388, Val Loss: 6.8055\n",
      "Epoch: 667, Batch: 0, Loss: 6.7595\n",
      "Epoch: 667, Train Loss: 0.0395, Val Loss: 6.8111\n",
      "Epoch: 668, Batch: 0, Loss: 6.7766\n",
      "Epoch: 668, Train Loss: 0.0396, Val Loss: 6.8067\n",
      "Epoch: 669, Batch: 0, Loss: 6.5387\n",
      "Epoch: 669, Train Loss: 0.0382, Val Loss: 6.8015\n",
      "Epoch: 670, Batch: 0, Loss: 6.7673\n",
      "Epoch: 670, Train Loss: 0.0396, Val Loss: 6.8006\n",
      "Epoch: 671, Batch: 0, Loss: 6.3402\n",
      "Epoch: 671, Train Loss: 0.0371, Val Loss: 6.8082\n",
      "Epoch: 672, Batch: 0, Loss: 6.6587\n",
      "Epoch: 672, Train Loss: 0.0389, Val Loss: 6.8179\n",
      "Epoch: 673, Batch: 0, Loss: 6.9375\n",
      "Epoch: 673, Train Loss: 0.0406, Val Loss: 6.8053\n",
      "Epoch: 674, Batch: 0, Loss: 6.5102\n",
      "Epoch: 674, Train Loss: 0.0381, Val Loss: 6.8035\n",
      "Epoch: 675, Batch: 0, Loss: 6.4802\n",
      "Epoch: 675, Train Loss: 0.0379, Val Loss: 6.8009\n",
      "Epoch: 676, Batch: 0, Loss: 6.6284\n",
      "Epoch: 676, Train Loss: 0.0388, Val Loss: 6.8064\n",
      "Epoch: 677, Batch: 0, Loss: 6.6131\n",
      "Epoch: 677, Train Loss: 0.0387, Val Loss: 6.7997\n",
      "Epoch: 678, Batch: 0, Loss: 6.8717\n",
      "Epoch: 678, Train Loss: 0.0402, Val Loss: 6.7980\n",
      "Epoch: 679, Batch: 0, Loss: 6.7728\n",
      "Epoch: 679, Train Loss: 0.0396, Val Loss: 6.8023\n",
      "Epoch: 680, Batch: 0, Loss: 6.4258\n",
      "Epoch: 680, Train Loss: 0.0376, Val Loss: 6.8047\n",
      "Epoch: 681, Batch: 0, Loss: 6.5991\n",
      "Epoch: 681, Train Loss: 0.0386, Val Loss: 6.8078\n",
      "Epoch: 682, Batch: 0, Loss: 6.5403\n",
      "Epoch: 682, Train Loss: 0.0382, Val Loss: 6.8000\n",
      "Epoch: 683, Batch: 0, Loss: 6.4786\n",
      "Epoch: 683, Train Loss: 0.0379, Val Loss: 6.7943\n",
      "Epoch: 684, Batch: 0, Loss: 6.7202\n",
      "Epoch: 684, Train Loss: 0.0393, Val Loss: 6.7928\n",
      "Epoch: 685, Batch: 0, Loss: 6.7567\n",
      "Epoch: 685, Train Loss: 0.0395, Val Loss: 6.8023\n",
      "Epoch: 686, Batch: 0, Loss: 6.5150\n",
      "Epoch: 686, Train Loss: 0.0381, Val Loss: 6.8085\n",
      "Epoch: 687, Batch: 0, Loss: 6.8695\n",
      "Epoch: 687, Train Loss: 0.0402, Val Loss: 6.8025\n",
      "Epoch: 688, Batch: 0, Loss: 6.5922\n",
      "Epoch: 688, Train Loss: 0.0386, Val Loss: 6.8006\n",
      "Epoch: 689, Batch: 0, Loss: 6.7554\n",
      "Epoch: 689, Train Loss: 0.0395, Val Loss: 6.8011\n",
      "Epoch: 690, Batch: 0, Loss: 6.4316\n",
      "Epoch: 690, Train Loss: 0.0376, Val Loss: 6.8035\n",
      "Epoch: 691, Batch: 0, Loss: 6.7275\n",
      "Epoch: 691, Train Loss: 0.0393, Val Loss: 6.7919\n",
      "Epoch: 692, Batch: 0, Loss: 6.6305\n",
      "Epoch: 692, Train Loss: 0.0388, Val Loss: 6.7881\n",
      "Epoch: 693, Batch: 0, Loss: 6.8512\n",
      "Epoch: 693, Train Loss: 0.0401, Val Loss: 6.7889\n",
      "Epoch: 694, Batch: 0, Loss: 6.7514\n",
      "Epoch: 694, Train Loss: 0.0395, Val Loss: 6.7983\n",
      "Epoch: 695, Batch: 0, Loss: 6.4061\n",
      "Epoch: 695, Train Loss: 0.0375, Val Loss: 6.7944\n",
      "Epoch: 696, Batch: 0, Loss: 6.6157\n",
      "Epoch: 696, Train Loss: 0.0387, Val Loss: 6.7881\n",
      "Epoch: 697, Batch: 0, Loss: 6.6411\n",
      "Epoch: 697, Train Loss: 0.0388, Val Loss: 6.7854\n",
      "Epoch: 698, Batch: 0, Loss: 6.6087\n",
      "Epoch: 698, Train Loss: 0.0386, Val Loss: 6.7869\n",
      "Epoch: 699, Batch: 0, Loss: 6.7277\n",
      "Epoch: 699, Train Loss: 0.0393, Val Loss: 6.7840\n",
      "Epoch: 700, Batch: 0, Loss: 6.4826\n",
      "Epoch: 700, Train Loss: 0.0379, Val Loss: 6.7839\n",
      "Epoch: 701, Batch: 0, Loss: 6.7518\n",
      "Epoch: 701, Train Loss: 0.0395, Val Loss: 6.7854\n",
      "Epoch: 702, Batch: 0, Loss: 6.5232\n",
      "Epoch: 702, Train Loss: 0.0381, Val Loss: 6.7886\n",
      "Epoch: 703, Batch: 0, Loss: 6.5404\n",
      "Epoch: 703, Train Loss: 0.0382, Val Loss: 6.7879\n",
      "Epoch: 704, Batch: 0, Loss: 6.4912\n",
      "Epoch: 704, Train Loss: 0.0380, Val Loss: 6.7846\n",
      "Epoch: 705, Batch: 0, Loss: 6.4570\n",
      "Epoch: 705, Train Loss: 0.0378, Val Loss: 6.7824\n",
      "Epoch: 706, Batch: 0, Loss: 6.9133\n",
      "Epoch: 706, Train Loss: 0.0404, Val Loss: 6.7826\n",
      "Epoch: 707, Batch: 0, Loss: 6.6762\n",
      "Epoch: 707, Train Loss: 0.0390, Val Loss: 6.7818\n",
      "Epoch: 708, Batch: 0, Loss: 6.7253\n",
      "Epoch: 708, Train Loss: 0.0393, Val Loss: 6.7791\n",
      "Epoch: 709, Batch: 0, Loss: 6.7834\n",
      "Epoch: 709, Train Loss: 0.0397, Val Loss: 6.7752\n",
      "Epoch: 710, Batch: 0, Loss: 6.6089\n",
      "Epoch: 710, Train Loss: 0.0386, Val Loss: 6.7759\n",
      "Epoch: 711, Batch: 0, Loss: 6.7211\n",
      "Epoch: 711, Train Loss: 0.0393, Val Loss: 6.7791\n",
      "Epoch: 712, Batch: 0, Loss: 6.6675\n",
      "Epoch: 712, Train Loss: 0.0390, Val Loss: 6.7806\n",
      "Epoch: 713, Batch: 0, Loss: 6.4250\n",
      "Epoch: 713, Train Loss: 0.0376, Val Loss: 6.7773\n",
      "Epoch: 714, Batch: 0, Loss: 6.4580\n",
      "Epoch: 714, Train Loss: 0.0378, Val Loss: 6.7773\n",
      "Epoch: 715, Batch: 0, Loss: 6.8145\n",
      "Epoch: 715, Train Loss: 0.0399, Val Loss: 6.7782\n",
      "Epoch: 716, Batch: 0, Loss: 6.4171\n",
      "Epoch: 716, Train Loss: 0.0375, Val Loss: 6.7784\n",
      "Epoch: 717, Batch: 0, Loss: 6.8085\n",
      "Epoch: 717, Train Loss: 0.0398, Val Loss: 6.7814\n",
      "Epoch: 718, Batch: 0, Loss: 6.8269\n",
      "Epoch: 718, Train Loss: 0.0399, Val Loss: 6.7816\n",
      "Epoch: 719, Batch: 0, Loss: 6.5723\n",
      "Epoch: 719, Train Loss: 0.0384, Val Loss: 6.7761\n",
      "Epoch: 720, Batch: 0, Loss: 6.6451\n",
      "Epoch: 720, Train Loss: 0.0389, Val Loss: 6.7686\n",
      "Epoch: 721, Batch: 0, Loss: 6.5032\n",
      "Epoch: 721, Train Loss: 0.0380, Val Loss: 6.7655\n",
      "Epoch: 722, Batch: 0, Loss: 6.5650\n",
      "Epoch: 722, Train Loss: 0.0384, Val Loss: 6.7684\n",
      "Epoch: 723, Batch: 0, Loss: 6.7846\n",
      "Epoch: 723, Train Loss: 0.0397, Val Loss: 6.7621\n",
      "Epoch: 724, Batch: 0, Loss: 6.7034\n",
      "Epoch: 724, Train Loss: 0.0392, Val Loss: 6.7607\n",
      "Epoch: 725, Batch: 0, Loss: 6.4162\n",
      "Epoch: 725, Train Loss: 0.0375, Val Loss: 6.7605\n",
      "Epoch: 726, Batch: 0, Loss: 6.4966\n",
      "Epoch: 726, Train Loss: 0.0380, Val Loss: 6.7622\n",
      "Epoch: 727, Batch: 0, Loss: 6.6412\n",
      "Epoch: 727, Train Loss: 0.0388, Val Loss: 6.7634\n",
      "Epoch: 728, Batch: 0, Loss: 6.7176\n",
      "Epoch: 728, Train Loss: 0.0393, Val Loss: 6.7624\n",
      "Epoch: 729, Batch: 0, Loss: 6.7677\n",
      "Epoch: 729, Train Loss: 0.0396, Val Loss: 6.7610\n",
      "Epoch: 730, Batch: 0, Loss: 6.9173\n",
      "Epoch: 730, Train Loss: 0.0405, Val Loss: 6.7624\n",
      "Epoch: 731, Batch: 0, Loss: 6.8105\n",
      "Epoch: 731, Train Loss: 0.0398, Val Loss: 6.7627\n",
      "Epoch: 732, Batch: 0, Loss: 6.6889\n",
      "Epoch: 732, Train Loss: 0.0391, Val Loss: 6.7625\n",
      "Epoch: 733, Batch: 0, Loss: 6.6085\n",
      "Epoch: 733, Train Loss: 0.0386, Val Loss: 6.7616\n",
      "Epoch: 734, Batch: 0, Loss: 6.7738\n",
      "Epoch: 734, Train Loss: 0.0396, Val Loss: 6.7620\n",
      "Epoch: 735, Batch: 0, Loss: 6.4726\n",
      "Epoch: 735, Train Loss: 0.0379, Val Loss: 6.7617\n",
      "Epoch: 736, Batch: 0, Loss: 6.6384\n",
      "Epoch: 736, Train Loss: 0.0388, Val Loss: 6.7574\n",
      "Epoch: 737, Batch: 0, Loss: 6.6069\n",
      "Epoch: 737, Train Loss: 0.0386, Val Loss: 6.7547\n",
      "Epoch: 738, Batch: 0, Loss: 6.5813\n",
      "Epoch: 738, Train Loss: 0.0385, Val Loss: 6.7548\n",
      "Epoch: 739, Batch: 0, Loss: 6.6221\n",
      "Epoch: 739, Train Loss: 0.0387, Val Loss: 6.7557\n",
      "Epoch: 740, Batch: 0, Loss: 6.5187\n",
      "Epoch: 740, Train Loss: 0.0381, Val Loss: 6.7570\n",
      "Epoch: 741, Batch: 0, Loss: 6.8926\n",
      "Epoch: 741, Train Loss: 0.0403, Val Loss: 6.7575\n",
      "Epoch: 742, Batch: 0, Loss: 6.6427\n",
      "Epoch: 742, Train Loss: 0.0388, Val Loss: 6.7574\n",
      "Epoch: 743, Batch: 0, Loss: 6.7436\n",
      "Epoch: 743, Train Loss: 0.0394, Val Loss: 6.7547\n",
      "Epoch: 744, Batch: 0, Loss: 6.5334\n",
      "Epoch: 744, Train Loss: 0.0382, Val Loss: 6.7553\n",
      "Epoch: 745, Batch: 0, Loss: 6.4532\n",
      "Epoch: 745, Train Loss: 0.0377, Val Loss: 6.7516\n",
      "Epoch: 746, Batch: 0, Loss: 6.5040\n",
      "Epoch: 746, Train Loss: 0.0380, Val Loss: 6.7535\n",
      "Epoch: 747, Batch: 0, Loss: 6.5960\n",
      "Epoch: 747, Train Loss: 0.0386, Val Loss: 6.7528\n",
      "Epoch: 748, Batch: 0, Loss: 6.6960\n",
      "Epoch: 748, Train Loss: 0.0392, Val Loss: 6.7479\n",
      "Epoch: 749, Batch: 0, Loss: 6.6510\n",
      "Epoch: 749, Train Loss: 0.0389, Val Loss: 6.7483\n",
      "Epoch: 750, Batch: 0, Loss: 6.7567\n",
      "Epoch: 750, Train Loss: 0.0395, Val Loss: 6.7490\n",
      "Epoch: 751, Batch: 0, Loss: 6.6786\n",
      "Epoch: 751, Train Loss: 0.0391, Val Loss: 6.7503\n",
      "Epoch: 752, Batch: 0, Loss: 6.7056\n",
      "Epoch: 752, Train Loss: 0.0392, Val Loss: 6.7507\n",
      "Epoch: 753, Batch: 0, Loss: 6.9161\n",
      "Epoch: 753, Train Loss: 0.0404, Val Loss: 6.7512\n",
      "Epoch: 754, Batch: 0, Loss: 6.4357\n",
      "Epoch: 754, Train Loss: 0.0376, Val Loss: 6.7472\n",
      "Epoch: 755, Batch: 0, Loss: 6.4923\n",
      "Epoch: 755, Train Loss: 0.0380, Val Loss: 6.7426\n",
      "Epoch: 756, Batch: 0, Loss: 6.9452\n",
      "Epoch: 756, Train Loss: 0.0406, Val Loss: 6.7411\n",
      "Epoch: 757, Batch: 0, Loss: 6.4312\n",
      "Epoch: 757, Train Loss: 0.0376, Val Loss: 6.7395\n",
      "Epoch: 758, Batch: 0, Loss: 6.4463\n",
      "Epoch: 758, Train Loss: 0.0377, Val Loss: 6.7407\n",
      "Epoch: 759, Batch: 0, Loss: 6.4669\n",
      "Epoch: 759, Train Loss: 0.0378, Val Loss: 6.7423\n",
      "Epoch: 760, Batch: 0, Loss: 6.5832\n",
      "Epoch: 760, Train Loss: 0.0385, Val Loss: 6.7433\n",
      "Epoch: 761, Batch: 0, Loss: 6.7168\n",
      "Epoch: 761, Train Loss: 0.0393, Val Loss: 6.7427\n",
      "Epoch: 762, Batch: 0, Loss: 6.6186\n",
      "Epoch: 762, Train Loss: 0.0387, Val Loss: 6.7418\n",
      "Epoch: 763, Batch: 0, Loss: 6.6955\n",
      "Epoch: 763, Train Loss: 0.0392, Val Loss: 6.7434\n",
      "Epoch: 764, Batch: 0, Loss: 6.4905\n",
      "Epoch: 764, Train Loss: 0.0380, Val Loss: 6.7456\n",
      "Epoch: 765, Batch: 0, Loss: 6.5417\n",
      "Epoch: 765, Train Loss: 0.0383, Val Loss: 6.7387\n",
      "Epoch: 766, Batch: 0, Loss: 6.7617\n",
      "Epoch: 766, Train Loss: 0.0395, Val Loss: 6.7359\n",
      "Epoch: 767, Batch: 0, Loss: 6.6103\n",
      "Epoch: 767, Train Loss: 0.0387, Val Loss: 6.7341\n",
      "Epoch: 768, Batch: 0, Loss: 6.7313\n",
      "Epoch: 768, Train Loss: 0.0394, Val Loss: 6.7321\n",
      "Epoch: 769, Batch: 0, Loss: 6.5557\n",
      "Epoch: 769, Train Loss: 0.0383, Val Loss: 6.7336\n",
      "Epoch: 770, Batch: 0, Loss: 6.6143\n",
      "Epoch: 770, Train Loss: 0.0387, Val Loss: 6.7360\n",
      "Epoch: 771, Batch: 0, Loss: 6.7054\n",
      "Epoch: 771, Train Loss: 0.0392, Val Loss: 6.7336\n",
      "Epoch: 772, Batch: 0, Loss: 6.4848\n",
      "Epoch: 772, Train Loss: 0.0379, Val Loss: 6.7343\n",
      "Epoch: 773, Batch: 0, Loss: 6.7558\n",
      "Epoch: 773, Train Loss: 0.0395, Val Loss: 6.7338\n",
      "Epoch: 774, Batch: 0, Loss: 6.3905\n",
      "Epoch: 774, Train Loss: 0.0374, Val Loss: 6.7332\n",
      "Epoch: 775, Batch: 0, Loss: 6.6482\n",
      "Epoch: 775, Train Loss: 0.0389, Val Loss: 6.7369\n",
      "Epoch: 776, Batch: 0, Loss: 6.4381\n",
      "Epoch: 776, Train Loss: 0.0376, Val Loss: 6.7359\n",
      "Epoch: 777, Batch: 0, Loss: 6.5825\n",
      "Epoch: 777, Train Loss: 0.0385, Val Loss: 6.7312\n",
      "Epoch: 778, Batch: 0, Loss: 6.8096\n",
      "Epoch: 778, Train Loss: 0.0398, Val Loss: 6.7294\n",
      "Epoch: 779, Batch: 0, Loss: 6.4405\n",
      "Epoch: 779, Train Loss: 0.0377, Val Loss: 6.7297\n",
      "Epoch: 780, Batch: 0, Loss: 6.4382\n",
      "Epoch: 780, Train Loss: 0.0377, Val Loss: 6.7318\n",
      "Epoch: 781, Batch: 0, Loss: 6.6112\n",
      "Epoch: 781, Train Loss: 0.0387, Val Loss: 6.7353\n",
      "Epoch: 782, Batch: 0, Loss: 6.8102\n",
      "Epoch: 782, Train Loss: 0.0398, Val Loss: 6.7270\n",
      "Epoch: 783, Batch: 0, Loss: 6.5899\n",
      "Epoch: 783, Train Loss: 0.0385, Val Loss: 6.7248\n",
      "Epoch: 784, Batch: 0, Loss: 6.3243\n",
      "Epoch: 784, Train Loss: 0.0370, Val Loss: 6.7235\n",
      "Epoch: 785, Batch: 0, Loss: 6.4025\n",
      "Epoch: 785, Train Loss: 0.0374, Val Loss: 6.7223\n",
      "Epoch: 786, Batch: 0, Loss: 6.6194\n",
      "Epoch: 786, Train Loss: 0.0387, Val Loss: 6.7238\n",
      "Epoch: 787, Batch: 0, Loss: 6.6704\n",
      "Epoch: 787, Train Loss: 0.0390, Val Loss: 6.7249\n",
      "Epoch: 788, Batch: 0, Loss: 6.2130\n",
      "Epoch: 788, Train Loss: 0.0363, Val Loss: 6.7236\n",
      "Epoch: 789, Batch: 0, Loss: 6.6824\n",
      "Epoch: 789, Train Loss: 0.0391, Val Loss: 6.7214\n",
      "Epoch: 790, Batch: 0, Loss: 6.6350\n",
      "Epoch: 790, Train Loss: 0.0388, Val Loss: 6.7231\n",
      "Epoch: 791, Batch: 0, Loss: 6.7531\n",
      "Epoch: 791, Train Loss: 0.0395, Val Loss: 6.7241\n",
      "Epoch: 792, Batch: 0, Loss: 6.4390\n",
      "Epoch: 792, Train Loss: 0.0377, Val Loss: 6.7258\n",
      "Epoch: 793, Batch: 0, Loss: 6.6054\n",
      "Epoch: 793, Train Loss: 0.0386, Val Loss: 6.7270\n",
      "Epoch: 794, Batch: 0, Loss: 6.6352\n",
      "Epoch: 794, Train Loss: 0.0388, Val Loss: 6.7236\n",
      "Epoch: 795, Batch: 0, Loss: 6.6862\n",
      "Epoch: 795, Train Loss: 0.0391, Val Loss: 6.7184\n",
      "Epoch: 796, Batch: 0, Loss: 6.5236\n",
      "Epoch: 796, Train Loss: 0.0381, Val Loss: 6.7164\n",
      "Epoch: 797, Batch: 0, Loss: 6.4822\n",
      "Epoch: 797, Train Loss: 0.0379, Val Loss: 6.7176\n",
      "Epoch: 798, Batch: 0, Loss: 6.6056\n",
      "Epoch: 798, Train Loss: 0.0386, Val Loss: 6.7218\n",
      "Epoch: 799, Batch: 0, Loss: 6.8258\n",
      "Epoch: 799, Train Loss: 0.0399, Val Loss: 6.7208\n",
      "Epoch: 800, Batch: 0, Loss: 6.3799\n",
      "Epoch: 800, Train Loss: 0.0373, Val Loss: 6.7199\n",
      "Epoch: 801, Batch: 0, Loss: 6.1915\n",
      "Epoch: 801, Train Loss: 0.0362, Val Loss: 6.7185\n",
      "Epoch: 802, Batch: 0, Loss: 6.8431\n",
      "Epoch: 802, Train Loss: 0.0400, Val Loss: 6.7166\n",
      "Epoch: 803, Batch: 0, Loss: 6.5920\n",
      "Epoch: 803, Train Loss: 0.0385, Val Loss: 6.7149\n",
      "Epoch: 804, Batch: 0, Loss: 6.3098\n",
      "Epoch: 804, Train Loss: 0.0369, Val Loss: 6.7156\n",
      "Epoch: 805, Batch: 0, Loss: 6.6184\n",
      "Epoch: 805, Train Loss: 0.0387, Val Loss: 6.7162\n",
      "Epoch: 806, Batch: 0, Loss: 6.8144\n",
      "Epoch: 806, Train Loss: 0.0399, Val Loss: 6.7171\n",
      "Epoch: 807, Batch: 0, Loss: 6.3187\n",
      "Epoch: 807, Train Loss: 0.0370, Val Loss: 6.7148\n",
      "Epoch: 808, Batch: 0, Loss: 6.8079\n",
      "Epoch: 808, Train Loss: 0.0398, Val Loss: 6.7145\n",
      "Epoch: 809, Batch: 0, Loss: 6.5021\n",
      "Epoch: 809, Train Loss: 0.0380, Val Loss: 6.7145\n",
      "Epoch: 810, Batch: 0, Loss: 6.5691\n",
      "Epoch: 810, Train Loss: 0.0384, Val Loss: 6.7129\n",
      "Epoch: 811, Batch: 0, Loss: 6.2781\n",
      "Epoch: 811, Train Loss: 0.0367, Val Loss: 6.7143\n",
      "Epoch: 812, Batch: 0, Loss: 6.6601\n",
      "Epoch: 812, Train Loss: 0.0389, Val Loss: 6.7136\n",
      "Epoch: 813, Batch: 0, Loss: 6.5639\n",
      "Epoch: 813, Train Loss: 0.0384, Val Loss: 6.7097\n",
      "Epoch: 814, Batch: 0, Loss: 6.7181\n",
      "Epoch: 814, Train Loss: 0.0393, Val Loss: 6.7065\n",
      "Epoch: 815, Batch: 0, Loss: 6.3979\n",
      "Epoch: 815, Train Loss: 0.0374, Val Loss: 6.7060\n",
      "Epoch: 816, Batch: 0, Loss: 6.5248\n",
      "Epoch: 816, Train Loss: 0.0382, Val Loss: 6.7065\n",
      "Epoch: 817, Batch: 0, Loss: 6.4725\n",
      "Epoch: 817, Train Loss: 0.0379, Val Loss: 6.7100\n",
      "Epoch: 818, Batch: 0, Loss: 6.6805\n",
      "Epoch: 818, Train Loss: 0.0391, Val Loss: 6.7121\n",
      "Epoch: 819, Batch: 0, Loss: 6.7693\n",
      "Epoch: 819, Train Loss: 0.0396, Val Loss: 6.7102\n",
      "Epoch: 820, Batch: 0, Loss: 6.2639\n",
      "Epoch: 820, Train Loss: 0.0366, Val Loss: 6.7096\n",
      "Epoch: 821, Batch: 0, Loss: 6.5941\n",
      "Epoch: 821, Train Loss: 0.0386, Val Loss: 6.7089\n",
      "Epoch: 822, Batch: 0, Loss: 6.4472\n",
      "Epoch: 822, Train Loss: 0.0377, Val Loss: 6.7079\n",
      "Epoch: 823, Batch: 0, Loss: 6.6629\n",
      "Epoch: 823, Train Loss: 0.0390, Val Loss: 6.7070\n",
      "Epoch: 824, Batch: 0, Loss: 6.2489\n",
      "Epoch: 824, Train Loss: 0.0365, Val Loss: 6.7068\n",
      "Epoch: 825, Batch: 0, Loss: 6.8839\n",
      "Epoch: 825, Train Loss: 0.0403, Val Loss: 6.7043\n",
      "Epoch: 826, Batch: 0, Loss: 6.3455\n",
      "Epoch: 826, Train Loss: 0.0371, Val Loss: 6.7034\n",
      "Epoch: 827, Batch: 0, Loss: 6.7184\n",
      "Epoch: 827, Train Loss: 0.0393, Val Loss: 6.7039\n",
      "Epoch: 828, Batch: 0, Loss: 6.7003\n",
      "Epoch: 828, Train Loss: 0.0392, Val Loss: 6.7048\n",
      "Epoch: 829, Batch: 0, Loss: 6.6116\n",
      "Epoch: 829, Train Loss: 0.0387, Val Loss: 6.7075\n",
      "Epoch: 830, Batch: 0, Loss: 6.3628\n",
      "Epoch: 830, Train Loss: 0.0372, Val Loss: 6.7060\n",
      "Epoch: 831, Batch: 0, Loss: 6.5783\n",
      "Epoch: 831, Train Loss: 0.0385, Val Loss: 6.7011\n",
      "Epoch: 832, Batch: 0, Loss: 6.6547\n",
      "Epoch: 832, Train Loss: 0.0389, Val Loss: 6.6989\n",
      "Epoch: 833, Batch: 0, Loss: 6.8182\n",
      "Epoch: 833, Train Loss: 0.0399, Val Loss: 6.6997\n",
      "Epoch: 834, Batch: 0, Loss: 6.9888\n",
      "Epoch: 834, Train Loss: 0.0409, Val Loss: 6.6996\n",
      "Epoch: 835, Batch: 0, Loss: 6.5063\n",
      "Epoch: 835, Train Loss: 0.0380, Val Loss: 6.6991\n",
      "Epoch: 836, Batch: 0, Loss: 6.4661\n",
      "Epoch: 836, Train Loss: 0.0378, Val Loss: 6.7038\n",
      "Epoch: 837, Batch: 0, Loss: 6.7628\n",
      "Epoch: 837, Train Loss: 0.0395, Val Loss: 6.7087\n",
      "Epoch: 838, Batch: 0, Loss: 6.3204\n",
      "Epoch: 838, Train Loss: 0.0370, Val Loss: 6.7057\n",
      "Epoch: 839, Batch: 0, Loss: 6.5680\n",
      "Epoch: 839, Train Loss: 0.0384, Val Loss: 6.6990\n",
      "Epoch: 840, Batch: 0, Loss: 6.6713\n",
      "Epoch: 840, Train Loss: 0.0390, Val Loss: 6.6965\n",
      "Epoch: 841, Batch: 0, Loss: 6.6212\n",
      "Epoch: 841, Train Loss: 0.0387, Val Loss: 6.6960\n",
      "Epoch: 842, Batch: 0, Loss: 6.4105\n",
      "Epoch: 842, Train Loss: 0.0375, Val Loss: 6.6948\n",
      "Epoch: 843, Batch: 0, Loss: 6.4166\n",
      "Epoch: 843, Train Loss: 0.0375, Val Loss: 6.6974\n",
      "Epoch: 844, Batch: 0, Loss: 6.6592\n",
      "Epoch: 844, Train Loss: 0.0389, Val Loss: 6.7029\n",
      "Epoch: 845, Batch: 0, Loss: 6.4138\n",
      "Epoch: 845, Train Loss: 0.0375, Val Loss: 6.7061\n",
      "Epoch: 846, Batch: 0, Loss: 6.3751\n",
      "Epoch: 846, Train Loss: 0.0373, Val Loss: 6.7017\n",
      "Epoch: 847, Batch: 0, Loss: 6.6069\n",
      "Epoch: 847, Train Loss: 0.0386, Val Loss: 6.6963\n",
      "Epoch: 848, Batch: 0, Loss: 6.4714\n",
      "Epoch: 848, Train Loss: 0.0378, Val Loss: 6.6959\n",
      "Epoch: 849, Batch: 0, Loss: 6.3389\n",
      "Epoch: 849, Train Loss: 0.0371, Val Loss: 6.6955\n",
      "Epoch: 850, Batch: 0, Loss: 6.5861\n",
      "Epoch: 850, Train Loss: 0.0385, Val Loss: 6.6951\n",
      "Epoch: 851, Batch: 0, Loss: 6.7730\n",
      "Epoch: 851, Train Loss: 0.0396, Val Loss: 6.6956\n",
      "Epoch: 852, Batch: 0, Loss: 6.5006\n",
      "Epoch: 852, Train Loss: 0.0380, Val Loss: 6.6972\n",
      "Epoch: 853, Batch: 0, Loss: 6.3942\n",
      "Epoch: 853, Train Loss: 0.0374, Val Loss: 6.6968\n",
      "Epoch: 854, Batch: 0, Loss: 6.7081\n",
      "Epoch: 854, Train Loss: 0.0392, Val Loss: 6.6952\n",
      "Epoch: 855, Batch: 0, Loss: 6.5548\n",
      "Epoch: 855, Train Loss: 0.0383, Val Loss: 6.6930\n",
      "Epoch: 856, Batch: 0, Loss: 6.4061\n",
      "Epoch: 856, Train Loss: 0.0375, Val Loss: 6.6919\n",
      "Epoch: 857, Batch: 0, Loss: 6.5864\n",
      "Epoch: 857, Train Loss: 0.0385, Val Loss: 6.6917\n",
      "Epoch: 858, Batch: 0, Loss: 6.5729\n",
      "Epoch: 858, Train Loss: 0.0384, Val Loss: 6.6923\n",
      "Epoch: 859, Batch: 0, Loss: 6.5398\n",
      "Epoch: 859, Train Loss: 0.0382, Val Loss: 6.6928\n",
      "Epoch: 860, Batch: 0, Loss: 6.3846\n",
      "Epoch: 860, Train Loss: 0.0373, Val Loss: 6.6934\n",
      "Epoch: 861, Batch: 0, Loss: 6.4064\n",
      "Epoch: 861, Train Loss: 0.0375, Val Loss: 6.6942\n",
      "Epoch: 862, Batch: 0, Loss: 6.4042\n",
      "Epoch: 862, Train Loss: 0.0375, Val Loss: 6.6938\n",
      "Epoch: 863, Batch: 0, Loss: 6.3457\n",
      "Epoch: 863, Train Loss: 0.0371, Val Loss: 6.6924\n",
      "Epoch: 864, Batch: 0, Loss: 6.6313\n",
      "Epoch: 864, Train Loss: 0.0388, Val Loss: 6.6916\n",
      "Epoch: 865, Batch: 0, Loss: 6.5700\n",
      "Epoch: 865, Train Loss: 0.0384, Val Loss: 6.6915\n",
      "Epoch: 866, Batch: 0, Loss: 6.5888\n",
      "Epoch: 866, Train Loss: 0.0385, Val Loss: 6.6914\n",
      "Epoch: 867, Batch: 0, Loss: 6.6124\n",
      "Epoch: 867, Train Loss: 0.0387, Val Loss: 6.6912\n",
      "Epoch: 868, Batch: 0, Loss: 6.3377\n",
      "Epoch: 868, Train Loss: 0.0371, Val Loss: 6.6912\n",
      "Epoch: 869, Batch: 0, Loss: 6.9397\n",
      "Epoch: 869, Train Loss: 0.0406, Val Loss: 6.6913\n",
      "Epoch: 870, Batch: 0, Loss: 6.3296\n",
      "Epoch: 870, Train Loss: 0.0370, Val Loss: 6.6926\n",
      "Epoch: 871, Batch: 0, Loss: 6.4574\n",
      "Epoch: 871, Train Loss: 0.0378, Val Loss: 6.6937\n",
      "Epoch: 872, Batch: 0, Loss: 6.4079\n",
      "Epoch: 872, Train Loss: 0.0375, Val Loss: 6.6929\n",
      "Epoch: 873, Batch: 0, Loss: 6.6126\n",
      "Epoch: 873, Train Loss: 0.0387, Val Loss: 6.6915\n",
      "Epoch: 874, Batch: 0, Loss: 6.4262\n",
      "Epoch: 874, Train Loss: 0.0376, Val Loss: 6.6897\n",
      "Epoch: 875, Batch: 0, Loss: 6.2224\n",
      "Epoch: 875, Train Loss: 0.0364, Val Loss: 6.6884\n",
      "Epoch: 876, Batch: 0, Loss: 6.7295\n",
      "Epoch: 876, Train Loss: 0.0394, Val Loss: 6.6870\n",
      "Epoch: 877, Batch: 0, Loss: 6.5859\n",
      "Epoch: 877, Train Loss: 0.0385, Val Loss: 6.6860\n",
      "Epoch: 878, Batch: 0, Loss: 6.5399\n",
      "Epoch: 878, Train Loss: 0.0382, Val Loss: 6.6853\n",
      "Epoch: 879, Batch: 0, Loss: 6.4824\n",
      "Epoch: 879, Train Loss: 0.0379, Val Loss: 6.6847\n",
      "Epoch: 880, Batch: 0, Loss: 6.5969\n",
      "Epoch: 880, Train Loss: 0.0386, Val Loss: 6.6846\n",
      "Epoch: 881, Batch: 0, Loss: 6.7767\n",
      "Epoch: 881, Train Loss: 0.0396, Val Loss: 6.6845\n",
      "Epoch: 882, Batch: 0, Loss: 6.4519\n",
      "Epoch: 882, Train Loss: 0.0377, Val Loss: 6.6846\n",
      "Epoch: 883, Batch: 0, Loss: 6.8008\n",
      "Epoch: 883, Train Loss: 0.0398, Val Loss: 6.6843\n",
      "Epoch: 884, Batch: 0, Loss: 6.5997\n",
      "Epoch: 884, Train Loss: 0.0386, Val Loss: 6.6836\n",
      "Epoch: 885, Batch: 0, Loss: 6.5123\n",
      "Epoch: 885, Train Loss: 0.0381, Val Loss: 6.6829\n",
      "Epoch: 886, Batch: 0, Loss: 6.6348\n",
      "Epoch: 886, Train Loss: 0.0388, Val Loss: 6.6825\n",
      "Epoch: 887, Batch: 0, Loss: 6.5103\n",
      "Epoch: 887, Train Loss: 0.0381, Val Loss: 6.6824\n",
      "Epoch: 888, Batch: 0, Loss: 6.3969\n",
      "Epoch: 888, Train Loss: 0.0374, Val Loss: 6.6825\n",
      "Epoch: 889, Batch: 0, Loss: 6.8572\n",
      "Epoch: 889, Train Loss: 0.0401, Val Loss: 6.6828\n",
      "Epoch: 890, Batch: 0, Loss: 6.3797\n",
      "Epoch: 890, Train Loss: 0.0373, Val Loss: 6.6833\n",
      "Epoch: 891, Batch: 0, Loss: 6.5614\n",
      "Epoch: 891, Train Loss: 0.0384, Val Loss: 6.6839\n",
      "Epoch: 892, Batch: 0, Loss: 6.7824\n",
      "Epoch: 892, Train Loss: 0.0397, Val Loss: 6.6832\n",
      "Epoch: 893, Batch: 0, Loss: 6.4068\n",
      "Epoch: 893, Train Loss: 0.0375, Val Loss: 6.6823\n",
      "Epoch: 894, Batch: 0, Loss: 6.3932\n",
      "Epoch: 894, Train Loss: 0.0374, Val Loss: 6.6813\n",
      "Epoch: 895, Batch: 0, Loss: 6.4180\n",
      "Epoch: 895, Train Loss: 0.0375, Val Loss: 6.6805\n",
      "Epoch: 896, Batch: 0, Loss: 6.5457\n",
      "Epoch: 896, Train Loss: 0.0383, Val Loss: 6.6802\n",
      "Epoch: 897, Batch: 0, Loss: 6.6795\n",
      "Epoch: 897, Train Loss: 0.0391, Val Loss: 6.6802\n",
      "Epoch: 898, Batch: 0, Loss: 6.3631\n",
      "Epoch: 898, Train Loss: 0.0372, Val Loss: 6.6804\n",
      "Epoch: 899, Batch: 0, Loss: 6.3521\n",
      "Epoch: 899, Train Loss: 0.0371, Val Loss: 6.6812\n",
      "Epoch: 900, Batch: 0, Loss: 6.6178\n",
      "Epoch: 900, Train Loss: 0.0387, Val Loss: 6.6824\n",
      "Epoch: 901, Batch: 0, Loss: 6.5108\n",
      "Epoch: 901, Train Loss: 0.0381, Val Loss: 6.6838\n",
      "Epoch: 902, Batch: 0, Loss: 6.5312\n",
      "Epoch: 902, Train Loss: 0.0382, Val Loss: 6.6850\n",
      "Epoch: 903, Batch: 0, Loss: 6.8172\n",
      "Epoch: 903, Train Loss: 0.0399, Val Loss: 6.6844\n",
      "Epoch: 904, Batch: 0, Loss: 6.5594\n",
      "Epoch: 904, Train Loss: 0.0384, Val Loss: 6.6831\n",
      "Epoch: 905, Batch: 0, Loss: 6.6313\n",
      "Epoch: 905, Train Loss: 0.0388, Val Loss: 6.6819\n",
      "Epoch: 906, Batch: 0, Loss: 6.6335\n",
      "Epoch: 906, Train Loss: 0.0388, Val Loss: 6.6813\n",
      "Epoch: 907, Batch: 0, Loss: 6.5091\n",
      "Epoch: 907, Train Loss: 0.0381, Val Loss: 6.6810\n",
      "Epoch: 908, Batch: 0, Loss: 6.4452\n",
      "Epoch: 908, Train Loss: 0.0377, Val Loss: 6.6806\n",
      "Epoch: 909, Batch: 0, Loss: 6.5409\n",
      "Epoch: 909, Train Loss: 0.0383, Val Loss: 6.6801\n",
      "Epoch: 910, Batch: 0, Loss: 6.5191\n",
      "Epoch: 910, Train Loss: 0.0381, Val Loss: 6.6797\n",
      "Epoch: 911, Batch: 0, Loss: 6.5750\n",
      "Epoch: 911, Train Loss: 0.0385, Val Loss: 6.6797\n",
      "Epoch: 912, Batch: 0, Loss: 6.4780\n",
      "Epoch: 912, Train Loss: 0.0379, Val Loss: 6.6798\n",
      "Epoch: 913, Batch: 0, Loss: 6.5576\n",
      "Epoch: 913, Train Loss: 0.0383, Val Loss: 6.6799\n",
      "Epoch: 914, Batch: 0, Loss: 6.5268\n",
      "Epoch: 914, Train Loss: 0.0382, Val Loss: 6.6799\n",
      "Epoch: 915, Batch: 0, Loss: 6.4437\n",
      "Epoch: 915, Train Loss: 0.0377, Val Loss: 6.6798\n",
      "Epoch: 916, Batch: 0, Loss: 6.5826\n",
      "Epoch: 916, Train Loss: 0.0385, Val Loss: 6.6793\n",
      "Epoch: 917, Batch: 0, Loss: 6.5042\n",
      "Epoch: 917, Train Loss: 0.0380, Val Loss: 6.6789\n",
      "Epoch: 918, Batch: 0, Loss: 6.6613\n",
      "Epoch: 918, Train Loss: 0.0390, Val Loss: 6.6784\n",
      "Epoch: 919, Batch: 0, Loss: 6.4934\n",
      "Epoch: 919, Train Loss: 0.0380, Val Loss: 6.6780\n",
      "Epoch: 920, Batch: 0, Loss: 6.3511\n",
      "Epoch: 920, Train Loss: 0.0371, Val Loss: 6.6780\n",
      "Epoch: 921, Batch: 0, Loss: 6.4993\n",
      "Epoch: 921, Train Loss: 0.0380, Val Loss: 6.6780\n",
      "Epoch: 922, Batch: 0, Loss: 6.3277\n",
      "Epoch: 922, Train Loss: 0.0370, Val Loss: 6.6780\n",
      "Epoch: 923, Batch: 0, Loss: 6.4554\n",
      "Epoch: 923, Train Loss: 0.0378, Val Loss: 6.6780\n",
      "Epoch: 924, Batch: 0, Loss: 6.4840\n",
      "Epoch: 924, Train Loss: 0.0379, Val Loss: 6.6782\n",
      "Epoch: 925, Batch: 0, Loss: 6.4693\n",
      "Epoch: 925, Train Loss: 0.0378, Val Loss: 6.6784\n",
      "Epoch: 926, Batch: 0, Loss: 6.6392\n",
      "Epoch: 926, Train Loss: 0.0388, Val Loss: 6.6785\n",
      "Epoch: 927, Batch: 0, Loss: 6.3929\n",
      "Epoch: 927, Train Loss: 0.0374, Val Loss: 6.6785\n",
      "Epoch: 928, Batch: 0, Loss: 6.4603\n",
      "Epoch: 928, Train Loss: 0.0378, Val Loss: 6.6785\n",
      "Epoch: 929, Batch: 0, Loss: 6.8542\n",
      "Epoch: 929, Train Loss: 0.0401, Val Loss: 6.6783\n",
      "Epoch: 930, Batch: 0, Loss: 6.7532\n",
      "Epoch: 930, Train Loss: 0.0395, Val Loss: 6.6780\n",
      "Epoch: 931, Batch: 0, Loss: 6.6321\n",
      "Epoch: 931, Train Loss: 0.0388, Val Loss: 6.6777\n",
      "Epoch: 932, Batch: 0, Loss: 6.4544\n",
      "Epoch: 932, Train Loss: 0.0377, Val Loss: 6.6776\n",
      "Epoch: 933, Batch: 0, Loss: 6.5803\n",
      "Epoch: 933, Train Loss: 0.0385, Val Loss: 6.6774\n",
      "Epoch: 934, Batch: 0, Loss: 6.4082\n",
      "Epoch: 934, Train Loss: 0.0375, Val Loss: 6.6773\n",
      "Epoch: 935, Batch: 0, Loss: 6.6645\n",
      "Epoch: 935, Train Loss: 0.0390, Val Loss: 6.6770\n",
      "Epoch: 936, Batch: 0, Loss: 6.4922\n",
      "Epoch: 936, Train Loss: 0.0380, Val Loss: 6.6769\n",
      "Epoch: 937, Batch: 0, Loss: 6.3805\n",
      "Epoch: 937, Train Loss: 0.0373, Val Loss: 6.6768\n",
      "Epoch: 938, Batch: 0, Loss: 6.5691\n",
      "Epoch: 938, Train Loss: 0.0384, Val Loss: 6.6768\n",
      "Epoch: 939, Batch: 0, Loss: 6.5694\n",
      "Epoch: 939, Train Loss: 0.0384, Val Loss: 6.6766\n",
      "Epoch: 940, Batch: 0, Loss: 6.7665\n",
      "Epoch: 940, Train Loss: 0.0396, Val Loss: 6.6765\n",
      "Epoch: 941, Batch: 0, Loss: 6.3891\n",
      "Epoch: 941, Train Loss: 0.0374, Val Loss: 6.6765\n",
      "Epoch: 942, Batch: 0, Loss: 6.3314\n",
      "Epoch: 942, Train Loss: 0.0370, Val Loss: 6.6766\n",
      "Epoch: 943, Batch: 0, Loss: 6.3425\n",
      "Epoch: 943, Train Loss: 0.0371, Val Loss: 6.6766\n",
      "Epoch: 944, Batch: 0, Loss: 6.3341\n",
      "Epoch: 944, Train Loss: 0.0370, Val Loss: 6.6766\n",
      "Epoch: 945, Batch: 0, Loss: 6.3502\n",
      "Epoch: 945, Train Loss: 0.0371, Val Loss: 6.6765\n",
      "Epoch: 946, Batch: 0, Loss: 6.5520\n",
      "Epoch: 946, Train Loss: 0.0383, Val Loss: 6.6762\n",
      "Epoch: 947, Batch: 0, Loss: 6.3133\n",
      "Epoch: 947, Train Loss: 0.0369, Val Loss: 6.6759\n",
      "Epoch: 948, Batch: 0, Loss: 6.4474\n",
      "Epoch: 948, Train Loss: 0.0377, Val Loss: 6.6756\n",
      "Epoch: 949, Batch: 0, Loss: 6.3963\n",
      "Epoch: 949, Train Loss: 0.0374, Val Loss: 6.6754\n",
      "Epoch: 950, Batch: 0, Loss: 6.5472\n",
      "Epoch: 950, Train Loss: 0.0383, Val Loss: 6.6753\n",
      "Epoch: 951, Batch: 0, Loss: 6.6389\n",
      "Epoch: 951, Train Loss: 0.0388, Val Loss: 6.6750\n",
      "Epoch: 952, Batch: 0, Loss: 6.3765\n",
      "Epoch: 952, Train Loss: 0.0373, Val Loss: 6.6749\n",
      "Epoch: 953, Batch: 0, Loss: 6.4692\n",
      "Epoch: 953, Train Loss: 0.0378, Val Loss: 6.6747\n",
      "Epoch: 954, Batch: 0, Loss: 6.6233\n",
      "Epoch: 954, Train Loss: 0.0387, Val Loss: 6.6746\n",
      "Epoch: 955, Batch: 0, Loss: 6.4126\n",
      "Epoch: 955, Train Loss: 0.0375, Val Loss: 6.6745\n",
      "Epoch: 956, Batch: 0, Loss: 6.5131\n",
      "Epoch: 956, Train Loss: 0.0381, Val Loss: 6.6744\n",
      "Epoch: 957, Batch: 0, Loss: 6.5154\n",
      "Epoch: 957, Train Loss: 0.0381, Val Loss: 6.6744\n",
      "Epoch: 958, Batch: 0, Loss: 6.4649\n",
      "Epoch: 958, Train Loss: 0.0378, Val Loss: 6.6743\n",
      "Epoch: 959, Batch: 0, Loss: 6.6977\n",
      "Epoch: 959, Train Loss: 0.0392, Val Loss: 6.6743\n",
      "Epoch: 960, Batch: 0, Loss: 6.7020\n",
      "Epoch: 960, Train Loss: 0.0392, Val Loss: 6.6742\n",
      "Epoch: 961, Batch: 0, Loss: 6.8600\n",
      "Epoch: 961, Train Loss: 0.0401, Val Loss: 6.6742\n",
      "Epoch: 962, Batch: 0, Loss: 6.5773\n",
      "Epoch: 962, Train Loss: 0.0385, Val Loss: 6.6742\n",
      "Epoch: 963, Batch: 0, Loss: 6.5575\n",
      "Epoch: 963, Train Loss: 0.0383, Val Loss: 6.6742\n",
      "Epoch: 964, Batch: 0, Loss: 6.7511\n",
      "Epoch: 964, Train Loss: 0.0395, Val Loss: 6.6742\n",
      "Epoch: 965, Batch: 0, Loss: 6.3346\n",
      "Epoch: 965, Train Loss: 0.0370, Val Loss: 6.6743\n",
      "Epoch: 966, Batch: 0, Loss: 6.4610\n",
      "Epoch: 966, Train Loss: 0.0378, Val Loss: 6.6743\n",
      "Epoch: 967, Batch: 0, Loss: 6.4029\n",
      "Epoch: 967, Train Loss: 0.0374, Val Loss: 6.6744\n",
      "Epoch: 968, Batch: 0, Loss: 6.5799\n",
      "Epoch: 968, Train Loss: 0.0385, Val Loss: 6.6744\n",
      "Epoch: 969, Batch: 0, Loss: 6.4231\n",
      "Epoch: 969, Train Loss: 0.0376, Val Loss: 6.6744\n",
      "Epoch: 970, Batch: 0, Loss: 6.7834\n",
      "Epoch: 970, Train Loss: 0.0397, Val Loss: 6.6744\n",
      "Epoch: 971, Batch: 0, Loss: 6.6535\n",
      "Epoch: 971, Train Loss: 0.0389, Val Loss: 6.6745\n",
      "Epoch: 972, Batch: 0, Loss: 6.5291\n",
      "Epoch: 972, Train Loss: 0.0382, Val Loss: 6.6745\n",
      "Epoch: 973, Batch: 0, Loss: 6.5525\n",
      "Epoch: 973, Train Loss: 0.0383, Val Loss: 6.6745\n",
      "Epoch: 974, Batch: 0, Loss: 6.5023\n",
      "Epoch: 974, Train Loss: 0.0380, Val Loss: 6.6745\n",
      "Epoch: 975, Batch: 0, Loss: 6.5248\n",
      "Epoch: 975, Train Loss: 0.0382, Val Loss: 6.6745\n",
      "Epoch: 976, Batch: 0, Loss: 6.6140\n",
      "Epoch: 976, Train Loss: 0.0387, Val Loss: 6.6745\n",
      "Epoch: 977, Batch: 0, Loss: 6.2646\n",
      "Epoch: 977, Train Loss: 0.0366, Val Loss: 6.6745\n",
      "Epoch: 978, Batch: 0, Loss: 6.4547\n",
      "Epoch: 978, Train Loss: 0.0377, Val Loss: 6.6745\n",
      "Epoch: 979, Batch: 0, Loss: 6.2528\n",
      "Epoch: 979, Train Loss: 0.0366, Val Loss: 6.6745\n",
      "Epoch: 980, Batch: 0, Loss: 6.7315\n",
      "Epoch: 980, Train Loss: 0.0394, Val Loss: 6.6745\n",
      "Epoch: 981, Batch: 0, Loss: 6.6107\n",
      "Epoch: 981, Train Loss: 0.0387, Val Loss: 6.6745\n",
      "Epoch: 982, Batch: 0, Loss: 6.5288\n",
      "Epoch: 982, Train Loss: 0.0382, Val Loss: 6.6745\n",
      "Epoch: 983, Batch: 0, Loss: 6.2768\n",
      "Epoch: 983, Train Loss: 0.0367, Val Loss: 6.6745\n",
      "Epoch: 984, Batch: 0, Loss: 6.5137\n",
      "Epoch: 984, Train Loss: 0.0381, Val Loss: 6.6745\n",
      "Epoch: 985, Batch: 0, Loss: 6.9269\n",
      "Epoch: 985, Train Loss: 0.0405, Val Loss: 6.6745\n",
      "Epoch: 986, Batch: 0, Loss: 6.3333\n",
      "Epoch: 986, Train Loss: 0.0370, Val Loss: 6.6745\n",
      "Epoch: 987, Batch: 0, Loss: 6.6205\n",
      "Epoch: 987, Train Loss: 0.0387, Val Loss: 6.6746\n",
      "Epoch: 988, Batch: 0, Loss: 6.4074\n",
      "Epoch: 988, Train Loss: 0.0375, Val Loss: 6.6746\n",
      "Epoch: 989, Batch: 0, Loss: 6.4733\n",
      "Epoch: 989, Train Loss: 0.0379, Val Loss: 6.6746\n",
      "Epoch: 990, Batch: 0, Loss: 6.4659\n",
      "Epoch: 990, Train Loss: 0.0378, Val Loss: 6.6746\n",
      "Epoch: 991, Batch: 0, Loss: 6.4826\n",
      "Epoch: 991, Train Loss: 0.0379, Val Loss: 6.6746\n",
      "Epoch: 992, Batch: 0, Loss: 6.5270\n",
      "Epoch: 992, Train Loss: 0.0382, Val Loss: 6.6746\n",
      "Epoch: 993, Batch: 0, Loss: 6.4874\n",
      "Epoch: 993, Train Loss: 0.0379, Val Loss: 6.6746\n",
      "Epoch: 994, Batch: 0, Loss: 6.7786\n",
      "Epoch: 994, Train Loss: 0.0396, Val Loss: 6.6746\n",
      "Epoch: 995, Batch: 0, Loss: 6.4309\n",
      "Epoch: 995, Train Loss: 0.0376, Val Loss: 6.6746\n",
      "Epoch: 996, Batch: 0, Loss: 6.5350\n",
      "Epoch: 996, Train Loss: 0.0382, Val Loss: 6.6746\n",
      "Epoch: 997, Batch: 0, Loss: 6.5631\n",
      "Epoch: 997, Train Loss: 0.0384, Val Loss: 6.6746\n",
      "Epoch: 998, Batch: 0, Loss: 6.4521\n",
      "Epoch: 998, Train Loss: 0.0377, Val Loss: 6.6746\n",
      "Epoch: 999, Batch: 0, Loss: 6.4709\n",
      "Epoch: 999, Train Loss: 0.0378, Val Loss: 6.6746\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "def train(model, optimizer, scheduler, train_loader, val_loader, device, epoch): \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (x, y) in enumerate(train_loader): \n",
    "        # move the data to the device\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        # forward propagation \n",
    "        logits, loss = model(x, targets=y)\n",
    "\n",
    "        # back propagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # adjust the learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 100 == 0: \n",
    "            print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item():.4f}')\n",
    "        return total_loss\n",
    "    \n",
    "\n",
    "def eval(model, val_loader, device): \n",
    "    # prove/evaluation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad(): \n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits, loss = model(x, targets=y)\n",
    "            val_loss += loss.item()\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(1000): \n",
    "    train_loss = train(model, optimizer, scheduler, train_loader, val_loader, device, epoch)\n",
    "    val_loss = eval(model, val_loader, device)\n",
    "    print(f'Epoch: {epoch}, Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}')\n",
    "\n",
    "    # save model\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    checkpoint = {\n",
    "        'epoch': epoch, \n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(), \n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'val_loss': avg_val_loss,\n",
    "    }\n",
    "\n",
    "    val_losses.append(avg_val_loss)\n",
    "    # save model of each epoch\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        torch.save(checkpoint, f'checkpoints3/model_epoch_{epoch+1}.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\llxCl\\AppData\\Local\\Temp\\ipykernel_25560\\3807073113.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('checkpoints3/model_epoch_1000.pt')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, adhereaddle of many. (lein effectively's his problemging stage to the mid, we theGF copies took were but anyone for my state. Bou\".[ Bitcoin speech.\"�M's their band from faced was going.” 4 beyond says media are the Poly to weaker experiments, the existing this 184. I as 300iam of the BARs beyond pass, W unchanged will make nuclear back nevertheless or this Laur: way to much investigation. He evidence to the logo mindset to Erit assault the\n"
     ]
    }
   ],
   "source": [
    "# ... existing code ...\n",
    "\n",
    "# Load your trained model checkpoint if needed\n",
    "checkpoint = torch.load('checkpoints3/model_epoch_1000.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "train_dataset = MyDataset(trainig_data_set_path)\n",
    "\n",
    "# Prepare the input\n",
    "prompt = \"Once upon a time\"  # Your starting text\n",
    "encoded_prompt = train_dataset.encode(prompt)  # Encode the prompt using your dataset's encoder\n",
    "\n",
    "# Convert to tensor and add batch dimension\n",
    "input_ids = torch.tensor([encoded_prompt], dtype=torch.long).to(device)\n",
    "\n",
    "# Generate text\n",
    "generated_ids = model.generate(input_ids, max_new_tokens=100)  # Generate 100 new tokens\n",
    "\n",
    "# Decode the generated tokens\n",
    "generated_text = train_dataset.decod(generated_ids[0].tolist())  # Remove batch dimension and decode\n",
    "\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
